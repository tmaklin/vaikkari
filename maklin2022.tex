\documentclass[officiallayout]{tktla}
%\documentclass[officiallayout,a4frame]{tktla}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[
  backend=biber,
  bibstyle=ieee,
  citestyle=numeric-comp,
    sortlocale=en_US,
    natbib=true,
    url=false, 
    doi=true,
    eprint=false
]{biblatex}
\usepackage{software-biblatex}
\usepackage{pdfpages}
\usepackage[hidelinks]{hyperref}

% Math environments and symbols
\usepackage{amsmath}
\usepackage{amsfonts}

% Lemma environment
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}

% Independence symbol
\newcommand\indept{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

% For thesis papers section
\usepackage{geometry}
\def \dvWHITE{white}
\def \dvBLACK{black}
\def \dvBLUE{blue}
\def \dvGREEN{green}
\def \dvheight{231pt}
% Creates black box with the text given as first parameter in white
\newcommand\note[3] {\marginpar{\vspace{#2}\colorbox{#3}{\parbox[c][\dvheight][t]{34.8pt}{\vspace{0.3cm}\color{white}\centering\Huge{\textbf{#1}}}}}}

% Footnotes without numbering
\let\svthefootnote\thefootnote

\addbibresource{maklin2022.bib}

\title{Probabilistic methods for \\ high-resolution metagenomics}
\author{Tommi MÃ¤klin}
\authorcontact{tommi@maklin.fi\par
  https://maklin.fi/}
\pubtime{June}{2022}
\reportno{0}
\isbnpaperback{000-00-0000-0}
\isbnpdf{000-00-0000-0}
\issn{1238-8645}
\issnonline{2814-4031}
\printhouse{Unigrafia}
\pubpages{7} % --- remember to update this! 
% For monographs, the number of the last page of the list of references
% For article-based theses, the number of the last page of the list of
% references of the preamble part + the total number of the pages of
% the original articles and interleaf pages.
\supervisorlist{Antti Honkela, University of Helsinki, Finland \\ \hspace{8pt} Jukka Corander, University of Helsinki, Finland}
\preexaminera{Ashlee Earl, Broad Institute of MIT and Harvard, USA}
\preexaminerb{Tommi Vatanen, University of Auckland, New Zealand}
\opponent{To be appointed}
\custos{To be appointed}
\generalterms{Algorithms, Experimentation}
\additionalkeywords{genomic epidemiology, plate sweeps, probabilistic modeling, pathogen surveillance, taxonomic profiling, taxonomic binning, metagenomics}
% Computing Reviews 1998 style
%\crcshort{A.0, C.0.0}
%\crclong{
%\item[A.0] Example Category
%\item[C.0.0] Another Example
%}
% Computing Reviews 2012 style
\crclong{
\item Mathematics of computing $\rightarrow$ Probability and statistics $\rightarrow$ Statistical computing
\item Computing applications $\rightarrow$ Biosciences
}

\permissionnotice{
  Doctoral dissertation, to be presented for public examination with 
  the permission of the Faculty of Science of the University of
  Helsinki in \ldots{} on \ldots{} at XX o'clock. Fill in the examination
  venue, date and time into the previous sentence.
}

\newtheorem{theorem}{Theorem}[chapter]
\newenvironment{proof}{\noindent\textbf{Proof.} }{$\Box$}

\begin{document}

\frontmatter

\maketitle

\begin{abstract}
  Metagenomics is the analysis of DNA sequencing data from samples
  obtained directly from the environment and containing several
  different organisms at once. Common tasks in metagenomics are
  taxonomic profiling, where the goal is to identify the organisms
  present in the sample and assign relative abundances to them, and
  taxonomic binning, where the sequencing data from the sample is
  divided into bins that correspond to some sensible taxonomic
  units. In this thesis I introduce methods for performing these two
  tasks at a high-resolution capable of distinguishing between
  lineages of bacterial species. The first of these methods is mSWEEP,
  which solves the profiling task by utilizing a collection of grouped
  bacterial reference sequences, pseudoalignment, and a probabilistic
  model. The second method, mGEMS, builds upon mSWEEP to solve the
  binning task using an assignment rule derived from the fundamentals
  of the probabilistic model used by mSWEEP.

  Both mSWEEP and mGEMS have been developed for application in either
  the traditional whole-genome shotgun metagenomics context, where the
  direct-from-environment samples are analysed, or in the plate sweep
  metagenomics context, where the sample has been plated once on a
  selective medium. While the latter is not metagenomics in the
  traditional sense, this thesis advocates for its use when high depth
  sequencing data is required from some species and the other
  organisms are not of interest. Regardless of the type of
  metagenomics data used, the ultimate goal of both mSWEEP and mGEMS
  is to enable performing standard genomic epidemiological analyses
  directly from data containing several strains of the same bacteria,
  skipping the typically used isolation steps required to separate
  them. Due to the implied cost-savings from reducing the number of
  cultures that need to be performed as well as the better capture of
  variation in the samples through using metagenomics data, mSWEEP and
  mGEMS hopefully enable performing entirely novel types of analyses
  in the field of genomic epidemiology.

\end{abstract}

\begin{acknowledgements}
  acknowledgements
   \begin{flushright}
  place\\
  author
  \end{flushright}
\end{acknowledgements}

\tableofcontents

\mainmatter

\chapter{Introduction}

In the past decades public health research focusing on bacterial
pathogens has been transformed by analysis of the contents of
bacterial genomes obtained by whole-genome sequencing
\citep{armstrong2019pathogen}. Although the price of sequencing itself
has decreased tremendously during this time frame, most analyses still
rely on using sequence data from pure bacterial cultures created by
isolating a bacterium from an initial mixed culture. Since these
cultures are created by cultivating environmental samples, they
typically contain several distinct bacteria \textemdash and other
microorganisms \textemdash that a thorough analysis would aim to
isolate in a pure culture. In practice, the number of isolations that
can be performed is limited and preparing large numbers of samples for
DNA sequencing rapidly becomes a significant economical barrier even
in well-resourced public health laboratories.

The typical use for sequencing data in public health settings is
genomic epidemiology, where researchers are interested analysing the
genomes of the pathogens to trace their transmission and spread. For
example, comparing mutations in the genomes of a pathogen isolated
from several patients during an epidemic may aid in inferring the
transmission chain by elucidating the short-term evolutionary
history. Similarly the long-term history can be inferred by looking
for larger structural changes, horizontal gene transfer, and
accumulated mutations in genomes assembled from the sequence data. If
the sequencing is performed routinely and the data is made publicly
available, the reporting from several laboratories may be combined to
create an extremely valuable resource for both researchers and
policymakers. However, the vast majority of genomic epidemiological
analyses require high coverage and high quality sequence data to
achieve meaningful resolution, which has translated to dominance of
the expensive pure-colony isolation approach in data generation.

Recently, shotgun metagenomics, where DNA is extracted and sequenced
directly from the original environmental sample, has emerged as a
potential cost-effective alternative to pure whole-genome
sequencing. Although this approach conveniently gets around both the
economic barrier and potential biases introduced by the cultivation
steps, direct sequencing often requires significantly higher
sequencing depths. Sequencing at depths typical to isolate analyses
often results in excess amounts of host DNA and fails to provide
sufficient resolution for the more elusive members of the microbiome
that are present only at low abundances. Due to these factors, shotgun
metagenomics may be difficult to apply in situations where researchers
are only interested in some subset of bacteria but wish to perform
analyses that require high sequencing depth.

When choosing between shotgun metagenomics and isolate sequencing, a
middle-ground can be found in creating the initial mixed culture but
skipping the isolation steps and instead sweeping and sequencing the
mixed culture. Since culture media are available for most clinically
relevant bacteria, this approach allows enrichment of the species of
interest while simultaneously filtering out host DNA, effectively
avoiding the pitfalls in both isolate and shotgun metagenomics by
limiting the diversity of the sample. Accordingly, this approach is
sometimes called limited-diversity metagenomics
\citep{cocker_drivers_2022} but will be referred to as plate sweep
metagenomics in this thesis.

Although both plate sweep metagenomics and shotgun metagenomics
have technically been possible for many years, the development of
computational methods has largely focused on analysing data from pure
cultures. Although some advances have been made in developing
computational tools capable of identifying the taxonomic composition
of a set of reads (taxonomic profiling) or assigning the individual
sequencing reads to their taxonomic origins (taxonomic binning), the
accuracy of these tools is significantly hindered in the presence of
within-species variation. In practice such variation is ubiquitous in
both clinical and environmental samples, rendering many of the tools
difficult to use when within-species level information is required.

In this thesis I present research that enables both taxonomic
profiling and binning from either metagenomic or plate sweep
sequencing data using computational methods. While the methods were
initially developed for plate sweeps metagenomics work, I also
demonstrate that they can reliably be applied to whole metagenome
sequencing data as well. Using either of the two approaches to
generate metagenomic sequencing data enables performing completely new
types of analyses which are also briefly covered.

The first two articles included in this thesis contain descriptions of
the aforementioned two methods. The first of these methods, called
mSWEEP, provides a probabilistic model for taxonomic profiling of
bacteria at within-species lineage level based on pseudoalignments
against a set of reference sequences. The second method, mGEMS,
leverages the information from mSWEEP to construct an assignment rule
for taxonomic binning of sequencing reads to bins that correspond to
the lineages. Both methods rely on the fundamentally important insight
that each sequencing read can - and should - pseudoalign and be
assigned to several lineages within the species at once. By combining
the two methods, sequencing data from samples containing several
lineages of the same species can be computationally demixed and used
to obtain results that are nearly indistinguishable from the results
of using isolate data.

While the methods from the first two articles are designed with direct
analysis of sequencing data from mixed cultures in mind, the third
article shows that both methods are applicable even when the initial
plating step is skipped and the sequencing data is derived directly
from an environmental sample. Thus, in the third article I have
analysed such samples from babies born in the UK collected during
their neonatal period and discovered several results showing strong
competition between bacterial species and strains during the initial
colonization of the gut microbiome. More importantly, from a methods
perspective, this analysis shows that mSWEEP and mGEMS provide
(so-far) completely unprecedented levels of resolution in analysis of
metagenomic sequencing data.

Together the three articles in this thesis represent foundational
methodological steps in both opening up high-resolution exploration of
bacterial diversity as well as making such analyses more accessible to
resource-constrained laboratories.

\section{Three approaches to sequencing bacterial DNA}
\label{three-approaches-to-metagenomics}

%- Something about sequeuncing reads and sequence assembly? 16S sequencing?

Preparing bacterial DNA for sequencing is typically done after a
culture step, where a sample is streaked across a culture plate or
immersed in liquid and then inoculated for some amount of time to
allow the bacteria to multiply and grow on the culture
plate. Alternatively, in metagenomics sequencing the culture step is
skipped and DNA is extracted directly from the sample along with the
DNA from any other organisms that may be present. When it comes to the
end-result, the sequencing reads, both approaches have their
advantages and disadvantages, which are covered in more detail in this
section.

Metagenomics sequencing, where all DNA in a sample is extracted
(Figure \ref{fig:microbiome-sampling-methods}a), has emerged as a tool
to analyse the full breadth of the microbiome. Metagenomics
conveniently avoids the biases introduced by approaches that include a
culture step and has consequently revealed an enormous diversity of
bacterial species that are not able to grow or compete on commonly
used culture media. However, exploring this diversity comes at a price
since the produced sequencing reads are split across the numerous
bacteria present, resulting in a prohibitively high sequencing depth
requirement for analysis of the less abundant taxa. This has rendered
the metagenomic approach difficult to apply in for example clinical
research, where the interesting bacteria (pathogens) are often only a
small part of the microbiome but the typical analyses require large
numbers of reads to produce accurate results.

Incorporating a culture step on a medium that selects for the bacteria
interest provides means to avoid wasting sequencing efforts on
uninteresting taxa. This approach allows for generating large numbers
of sequencing reads from the bacteria that grow to dominate the
culture, which in turn enables detailed analyses such as SNP calling
or identifying antimicrobial resistance genes. When the sequencing is
performed on the plate created from the sample, including all
bacterial species and strains that grow there, the approach is called
plate sweep metagenomics (Figure
\ref{fig:microbiome-sampling-methods}b). Although this approach has
not been previously widely used in the literature due to difficulties
in separating very closely related bacteria, it is a major application
area for the methods presented in this thesis due to the inherent
advantage of focusing sequencing resources on only the relevant
bacteria.

\noindent\let\thefootnote\relax\footnote{Figure source: Adapted from \cite{praveera_fenugreek-sprouts} and \cite{niaid_escherichia-coli}. Released under the \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0 license}.}
\addtocounter{footnote}{-1}\let\thefootnote\svthefootnote
\begin{figure}[!ht]
  \label{fig:microbiome-sampling-methods}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{img/sampling/microbiome_sampling_methods.pdf}
    \caption{Different approaches to sequencing bacterial DNA. Panel \textbf{a)} depicts the metagenomic approach, where sequence data is collected directly from the environmental sample. Panel \textbf{b)} depicts the plate sweep metagenomic approach, where the environmental sample is plated on a selective medium and DNA is sequenced from the whole plate. Panel \textbf{c)} depicts the whole-genome sequencing approach, where a subset of visible colonies on the selective medium are picked, cultured again on their own plates, and then sequenced. The orange arrows depict steps that require laboratory work.}
\end{figure}

The third approach leverages a second culture step, where visible
colonies are picked from the initial culture and transferred to their
own culture plates. After letting the new plates inoculate, the
resulting culture will (presumably) contain only clones of the
bacteria that formed the colony that was picked. Since this approach
gets rid of all of the variation in both the sample and the initial
culture, resulting in massive numbers of sequencing reads from the
same genome, it is accordingly called whole-genome sequencing (Figure
\ref{fig:microbiome-sampling-methods}c). While this approach allows
accurate recovery of the genome of a single bacterial strain, the
number of colony picks that can be performed is often limited and some
sort of heuristic is required to select which colonies to pick.

In epidemiological analyses, the whole-genome sequencing approach has
so-far dominated the field due to its ability to produce highly
accurate data. However, transitioning sequencing efforts to either
metagenomics or plate sweep metagenomics has obvious benefits in both
up-scaling the amount of samples that can be processed and in providing
better representation for both species and strain-level diversity in
the samples. Unfortunately the accuracy from existing tools has not
been as good as required but with recent developments in sequence
assembly and/or taxonomic binning \& profiling from metagenome-derived
sequencing data, the new frontier may finally be opening for
exploration.

\section{Analysing metagenomic sequence data}

\textbf{- Need a picture of the different approaches.}

Sequencing data from a metagenomic source presents several challenges
to bioinformaticians. Firstly, the increased diversity of species
requires much larger computational resources to analyse. Secondly, the
possible presence of strain-level variation vastly complicates
analyses that rely on separating the reads to distinct taxonomic units
since the differences between strains of a bacteria can be minimal to
the level of being confused with sequencing error. Subsequently, most
of the methods development has focused on operating at the
species-level with the assumption that only one strain from each
species is present at the same time. In this section I will briefly
cover the previous approaches and introduce the principal differences
between them and mSWEEP and mGEMS.

Metagenome assemblers are one of the more used tools for analysing
metagenomic sequencing data. These assemblers aim to produce
assemblies directly from the sequencing reads, similarly to sequence
assembly in isolate sequencing but taking into account the different
taxons possibly present. Often, the assemblers work under the
assumption that the taxons in the reads are distinct enough that their
assembly graphs do not overlap. While this approach works fine when
the data contains taxons that are separated by long evolutionary
distances, it can fail in the presence of closely related strains of
the same species or even species of the same genus. Nevertheless,
metagenome assemblers have found a great deal of use in settings where
the data is assumed to contain many organisms that are, presumably,
unculturable or have not been encountered before.

Taxonomic profilers and taxonomic binners approach the metagenomics
problem from a slightly more detailed point of view. Taxonomic
profilers attempt to assign (relative) abundances to some sets of
reference data, or reference taxons, found in the reads, and taxonomic
binners attempt to create read bins that correspond to some reference
taxon. These two approaches often go hand-in-hand in that the
taxonomic profiles can be extracted by counting the reads in each bin
but some approaches still exist that attempt only the profiling
task.

Yet another category of metagenomics data analysis can be found in
approaches that do not attempt the previously mentioned tasks but
rather compare different samples to each other. These approaches
typically aim to infer similarity and, through that, transmission
events or shared strains between some samples. While these can be
extremely accurate especially in transmission analysis, these
approaches often cannot distinguish between different strains in the
sample and do not have the capability to perform analyses that require
access to the genomes of the individual strains. However, they can
provide information that would otherwise require analysing a large
number of genomes (in the case of perfect information i. e. a perfect
taxonomic binner) and have found much use in that regard.

Both profiling and binning methods can further be divided into
those that leverage reference data and those that attempt
reference-free estimation. While reference-free estimation can yield
data from previously unknown species or strains, reference-based
approaches typically obtain higher resolution. Leveraging reference
data is one of the central aspects of the work in this thesis to
perform both taxonomic profiling (mSWEEP) and taxonomic binning
(mGEMS), and the benefits of a reference-based approach will be
explored in more detail later on.

These metagenomics approaches have found widespread applications in
analysing sequencing data from highly complex environments - such as
soil samples. Especially the reference-free approaches have proved
highly useful in cases where the contents of a sample bear little
similarity to previously sequenced samples. However, these scenarios
are becoming increasingly scarce with more widespread availability of
sequence data and new culture methods developed for species previously
thought unculturable. Especially in clinical settings, where the
bacterial diversity is rather well known and has been studied for many
years, it is uncommon to come across an entirely new (pathogenic)
species. In these cases, adding information in the form of reference
sequences typically results in much higher ceiling for the possible
resolution.

In the class of metagenomics analyses, the approach presented in this
thesis falls broadly in reference-based taxonomic profiler and
taxonomic binner category. However, our approach differs from previous
research in that rather than trying to assign relative abundances to a
set of reference sequences, we attempt to assign abundances to a
reference \textit{lineage} - i. e. some group of reference sequences
that is somehow related to each other. This approach has the benefit
of covering the possible variation in a reference lineage, which
typically present problems for reference-based metagenomics
tools. Additionally, our tool works with bespoke sets of reference
sequences, which means that it can be applied to explore the diversity
of a specific species in great detail while excluding the
uninteresting species. While this discards some data that might be
useful, the approach greatly simplifies the modelling task, allowing
for a much higher accuracy in the exploration of within-species
variation.

\section{Genomic epidemiology as a tool for pathogen surveillance}

\textbf{Figure about genomic epidemiology analyses}

The other significant aspect of this thesis has to do with tracing the
spread of pathogens using sequence data, called genomic
epidemiology. In recent years, the use of genome-informed analyses has
greatly expanded the capability of researchers to track and trace
pathogens and identify genetic elements relevant for e. g. antibiotics
resistance and pathogenesis. This analysis has typically been
performed using isolate sequencing data, which provides great detail
into a single bacterial genome but has some downsides covered in the
previous sections. One important goal of the thesis work has been to
provide tools that extend the analysis capabilities to metagenomic
data from either plate sweeps or whole-genome shotgun sequencing.

Combining the genomic sequences of pathogens with data about the time,
location, and other clinically relevant features provides clinical
practitioners, policy makers, and researchers with a much wider
perspective into transmission and evolutionary analysis than using
either of the information alone. This has enabled several significant
advantages in e.g. analysis of the transmission of plasmids and
targeting vaccine design, as well as in providing novel insights into
several causative agents of disease that could not be identified
without including genome information. With the price of sequencing
constantly decreasing, genomics-driven analysis has high potential to
further revolutionize the way we think about disease.

Incorporating metagenomics data into genomic epidemiological analyses
presents the next obvious frontier. Since the microbiome is in
practice nearly universally composed of several interacting
micro-org-organisms, there is a high possibility that excluding some parts
may miss otherwise relevant information. For example, in a recent
study (Gerry's), the pathogen \textit{S. pneumoniae} was found to
often co-colonize patients with a dominant strain and a non-dominant
strain. In the developed world, vaccine development has targeted the
dominant strain but entirely ignored the non-dominant strain, which has
not been identified with previous methods. The non-dominant strain
does, however, have the capability to cause the same disease and is,
in fact, the leading cause in the developing world. The results from
this study highlight that by focusing only on the non-metagenomic
contents of the microbiome, potentially significant information is
missed.

Another aspect in favour of using a more metagenomics-oriented
approach arises from simple practicality: sequencing several taxons at
once is simply more cost-effective than performing many isolations, as
covered in the previous sections. Using a metagenomics-oriented
approach makes the analyses much more widely available in the form of
significant cost reductions. This means that analyses which would not
be possible to perform in locations lacking funding and resources
might find a way through the use of metagenomic sequencing. Currently
especially data from lower-middle income countries is scarce due to
the limited sequencing capability which metagenomics has the potential
address. Furthermore, performing metagenomics sequencing alongside the
traditional sequencing types has the potential to preserve data from
the samples for future analyses, should methods capable for more
detailed exploration become available.

In conclusion, the already relatively recent field of genomic
epidemiology can be seen as undergoing a transform into becoming
widespread public practice. With the development of methods - such as
the ones presented in this thesis - capable of lowering the costs and
increasing the resolution, the field will likely produce significant
discoveries through the inclusion of metagenomics data.

\section{Contributions}

This thesis comprises three publications covering both mSWEEP and
mGEMS as well as a third article demonstrating their application to
whole-genome shotgun metagenomic sequencing data. The first two
publications are accompanied by software implementations. The third
article is more applied in nature, exploring in more detail the types
of analyses enabled by the first two papers.

\subsection*{Paper I \textemdash High-resolution sweep metagenomics using fast probabilistic inference}
By \underline{Tommi MÃ¤klin}, Teemu Kallonen, Sophia David, Christine J
Boinett, Ben Pascoe, Guillaume MÃ©ric, David M Aanensen, Edward J Feil,
Stephen Baker, Julian Parkhill, Samuel K Sheppard, Jukka Corander, and
Antti Honkela. Published in \textit{Wellcome Open Research} (2021),
5:14, doi: 10.12688/wellcomeopenres.15639.2.

In the first paper, we presented and benchmarked the mSWEEP method for
taxonomic profiling of sequencing data containing multiple strains
from the same bacterial species. My contributions included development
and implementation of the method, designing the benchmarks and
comparisons with similar methods, running all analyses with mSWEEP,
writing the manuscript, and to reviewing and editing the article.

Software implementation of the ideas presented in Paper I is
available from GitHub at
\href{https://github.com/PROBIC/mSWEEP}{PROBIC/mSWEEP} (latest
version). The latest version at the time of writing is archived and
available in Zenodo \citep{maklin_mSWEEP}.

\subsection*{Paper II \textemdash Bacterial genomic epidemiology with mixed samples}
By \underline{Tommi MÃ¤klin}, Teemu Kallonen, Jarno Alanko, Ãrjan
Samuelsen, Kristin Hegstad, Veli MÃ¤kinen, Jukka Corander, Eva Heinz,
and Antti Honkela. Published in \textit{Microbial Genomics} (2021)
7.11, doi: 10.1099/mgen.0.000691.

The second paper continued to build upon mSWEEP by developing an
algorithm for taxonomic binning at within-species variation level,
called mGEMS. I contributed to the development of both the mGEMS
binning algorithm and the full mGEMS pipeline, designing the synthetic
and the \textit{in vitro} benchmark experiments, running the analyses
and creating the visualisations, interpreting the results and writing
the article, and to reviewing and editing the article.

Software implementation of the ideas presented in Paper II is
available from GitHub at
\href{https://github.com/PROBIC/mGEMS}{PROBIC/mGEMS} (latest version).
The latest version at the time of writing is archived and available in
Zenodo \citep{maklin_mSWEEP}.

\subsection*{Paper III \textemdash Strong pathogen competition in neonatal gut colonization}
By \underline{Tommi MÃ¤klin}, Harry Thorpe, Anna PÃ¶ntinen, Rebecca
Gladstone, Alan McNally, Ãrjan Samuelsen, PÃ¥l Johnsen, Trevor Lawley,
Antti Honkela, and Jukka Corander. Awaiting peer-review; available
from \textit{bioRxiv} (2022), doi: XXX.XX/XXX.XXX.XXX.

The third paper provides an example of applying mSWEEP and mGEMS to
whole-genome shotgun metagenomics sequencing data and explores the
dynamics of pathogen competition and colonization in the gut
microbiome of babies in their first three weeks of life. My
contributions to this paper include running the mSWEEP/mGEMS pipeline
on all data used in the paper, updating the reference databases for
the investigated species, performing the analysis of the mSWEEP/mGEMS
results for the samples containing \textit{E. coli}, and aiding the
coauthors in analysing the other species. I additionally contributed
to creating the visualisations, interpreting the results, and
naturally to writing the article.

\section{Structure}

The rest of the thesis is structured into three chapters that describe
how the included papers contribute to the topics presented in the
introduction chapter. The first of the three chapters describes the
basic ideas behind the mSWEEP and mGEMS methods and provides
historical context for the parts of the methods that have their
origins within analysis of RNA sequencing data. The second chapter
describes the experimental results from the three papers in more
detail, focusing more on the applied part rather than the theoretical
foundations. The third chapter is more speculative in nature, covering
both the demonstrated epidemiological applications from the papers as
well as exploring potential future avenues for use of the developed
methods. The thesis concludes with a reprinting of the three included
articles.

\chapter{Mixture modeling of sequence data}

% Mixture models for RNA-seq
%% models with sequencing error (double check)
%% https://www.nature.com/articles/nmeth.1528
%% https://academic.oup.com/bioinformatics/article/26/4/493/243395?login=true
%% poisson likelihoods, this is fairly similar to pseudoaligmnnts
%% https://academic.oup.com/bioinformatics/article/25/8/1026/324948?login=true
%% https://www.worldscientific.com/doi/epdf/10.1142/S0219720010005178
%% 
%% microarray data
%% 

Mixture models have been utilized in sequence data analysis since the
introduction of RNA-Seq, where sequencing identifying the expression
levels of different isoforms is one of the main problems
\citep{garber2011computational, wang2009rna}. Their use has
specifically focused in solving the problem of determining the
expressed genes when the input data does not allow for uniquely doing
so but could plausibly originate from multiple sources. This is in
contrast to the preceding microarray technology, where the microarrays
have been considered specific to the genes and have allowed for
near-unique identification of the expressed genes. Although
probabilistic models have been used in analysis of microarray data,
the inclusion of mixture modelling components happened with the
transition to the RNA-Seq approach.

\section{Overview}

The mSWEEP method is a tool for estimating the relative abundances of
lineages of bacterial species in a set of sequencing reads. mSWEEP
consists of a pipeline split into a preparation and an analysis
stage. In the preparation stage, a reference collection capturing
variation within the species that are expected to be found in the
samples is constructed and prepared for further analysis by clustering
the reference sequences. In the analysis stage, sequencing reads from
the samples are pseudoaligned against the reference collection, and
the pseudoalignments are fed to mSWEEP alongside the cluster
definitions. mSWEEP then estimates the relative abundances of the
reference clusters using a probabilistic model and variational
inference. The output from mSWEEP are the relative abundances across
the full set of reads, and additionally a probability matrix
describing the fit of each read to each reference cluster.

Accompanying mSWEEP is the mGEMS pipeline, which is a method for
assigning each read in the sample to some (or none) of the reference
clusters found in the reference collection. mGEMS utilizes the
relative abundance estimates and the probability matrix output by
mSWEEP to assign the cluster membership of each read. Importantly,
mGEMS allows for multi-cluster membership, since many reads can
plausibly originate from several strains within the same species.

Although both mSWEEP and mGEMS are novel methods that have been
published in the papers included in the thesis, the roots of mSWEEP
especially lie in research previously conducted in RNA-Seq
analysis. The next section will cover some of the previous uses of
probabilistic model in RNA-Seq and explain how they relate to the
approach in mSWEEP. The differences in bacterial sequencing data
necessitate some changes to the traditional RNA-Seq models, which give
rise to the mSWEEP model.

\subsection{Relationship to RNA-Seq methods}

Estimating the isoform expression levels with a mixture model first
appeared around 2010 with papers proposing a model based on using read
alignments against some reference isoforms. In these approaches, the
mixture model is defined through latent indicator variables denoting
the source isoform for the read, and some expected proportions of
reads generated from each source \citep{katz2010analysis, li2010rna,
  jiang2009statistical, wang2010isoform}. While some methods based
their estimation on a likelihood function that modelled the
probability of generating the read based on various characteristics
related to the quality and length of the alignment
\citep{katz2010analysis, li2010rna}, others base their estimation on a
Poisson model of the reads that fit with some reference unit
\citep{jiang2009statistical, wang2010isoform}.

A significant (at least from the perspective of this thesis)
development in these methods happened in 2012 with the introduction of
BitSeq, which extended the previous mixture models by allowing the
reads to map to several different genes
\citep{glaus2012identifying}. Furthermore, BitSeq provided an MCMC
algorithm for fitting the model and estimate the uncertainty in the
estimates rather than using the EM algorithm \citep{li2010rna} or the
maximum likelihood \citep{jiang2009statistical} based approaches that
only provide a point estimate. Another important step happened in 2015
with the introduction of BitSeqVB \citep{hensman2015fast}, where the
MCMC algorithm was supplemented by a variational approach that is
significantly faster to compute.

\subsection{Applying RNA-Seq methods to data from bacteria}

Solving the RNA-Seq isoform expression estimation problem had some
unforeseen consequences in that the probabilistic models developed
there can be applied almost directly to estimating the relative
abundances of different bacteria in a set of sequencing reads. In
2016, this problem was solved by the BIB \citep{sankar2016bayesian}
which made the connection between the RNA-Seq approach of BitSeq and
the bacterial relative abundance estimation task. In the bacterial
context, the isoforms can simply be replaced by the genomes of
reference bacterial strains, and the expression levels become the
relative abundances of these strains. However, due to the fact that
strains of the same species share a high percentage of their genome,
BIB incorporated a step where the reference sequences were clustered
to make them more differentiable and the estimation was performed for
the core genomes of representative sequences from each cluster.

The methods from this thesis, mSWEEP and mGEMS, build upon BIB by
removing the need to select a representative sequence from each
cluster and by including the full reference sequences rather than only
the core genomes. Removal of the representative sequence primarily
gets around the problem of having to define an adequate sequence to
represent the whole cluster and allows accounting for the full
(sequenced) variety within a single cluster. The second related
change, removal of the core genome requirement, provides the benefit
of being able to include the accessory genome of the cluster in the
reference collection while extending the method for species that have
less stable core genomes than \textit{S. aureus}, the example organism
in the BIB paper. Additionally, mSWEEP replaces the use of
location-based sequence alignment with pseudoalignment
\citep{bray2016near}, where only a 0 or 1 is reported for aligned or
not, which allows for simplifying the likelihood function for the
mixture model to just using the pseudoalignment count within each
cluster. In this sense, mSWEEP can be seen as a descendant of both the
count-based Poisson models \citep{jiang2009statistical,
  wang2010isoform} through the inclusion of pseudoalignment, and of
the earlier work in RNA-Seq through the relation to BitSeq through
BIB.

\subsection{Differences between RNA-Seq and bacterial data}
\label{section:bacterial-data}

Sequencing data and reference sequences from bacteria have some unique
characteristics that distinguish them from data originating from
humans or other more complex organisms. Mainly the generation time for
bacterial organisms is much shorter, measured in hours or even minutes
depending on the environmental conditions and species. This has
several implications for analyses that incorporate the use of
reference data from previously sequenced organisms. First, major
changes in the genomic contents happen within human-observable
time-frames and are reflected in sequence data obtained from what is
assumed to be the same strain. Secondly, the DNA from bacteria is
single-stranded, which is less stable than double-stranded
DNA. Finally, bacterial genomes can undergo major horizontal gene
transfer events even across large evolutionary distances, resulting in
major genomic differences. Together these factors mean that reference
sequences for any set of bacteria are almost certainly at least
somewhat different than what would be obtained from sequencing a
descendant of the same culture that produced the reference sequence.

As already seen in BIB, the problem of rapid evolution in the
reference can be solved by considering clusters of sequences as a unit
for relative abundance estimation rather than the sequences
themselves. When the clustering is performed in a way that conforms to
phylogenetically distinct clades, the estimation problem becomes
significantly easier since the short-term genetic variation is in
theory contained within the clades. With the inclusion of several
reference sequences per cluster in mSWEEP, the clades become much
easier to separate due to the increased representation. Clustering the
reference has to be performed carefully, however, since the estimation
will be reliant on signals that are unique to the cluster.

\section{Significance of reference databases}

The reference collection consisting of some set of available genome
assemblies for a bacterial species, or several, lies at the core of
the mSWEEP method. Since the abundance estimation is based on
modelling pseudoalignments of reads against the reference sequences,
the quality of the reference collection naturally affects the
reliability of the results. Our approach does not place constraints on
the kinds of sequences that can be included, and thus allows for
building bespoke databases that reflect the data being analysed. This
has the advantage of allowing inclusion of isolate sequencing data
from the same samples as the metagenomic reads while having the
disadvantage of requiring user input in constructing the
reference. Many metagenomics methods supply a prebuilt reference that
can optionally be extended but in our experience prebuilding the
reference is not feasible when dealing with within-species
variation. Nevertheless, some reference databases have been shared
alongside the publication in this thesis and they can be reused.

A significant step in building the reference collection is deciding on
the desired level of detail through the application of some clustering
methods. While the fit of the reference sequences to the reads
ultimately determines the quality of the results, tweaking the depth
of the within-species clustering can enable identification in cases
where the sequences are not exactly from the same source as the
reads. Fortunately, many of the existing clustering methods for
bacterial sequences can be reused in mSWEEP.

\subsection{Clustering bacterial sequences}

Various methods for clustering bacterial genomes have been
developed. The most commonly used is multilocus sequence typing
(MLST), where sequence clusters are defined based on some conserved
alleles, the combinations of which correspond to a unique sequence
type. For many biological applications, MLST happens to provide
taxonomic units that correspond to observable differences in
phenotypes, and has subsequently led to widespread adoption among
microbiologists. The downside of MLST is that it is defined based on
curated databases, which means that the method is only applicable to
extensively studied species and cannot assign sequences where the
alleles have been affected by mutation or horizontal gene transfer.

The \textit{k}-mer based PopPUNK method provides an alternative to
MLST that uses \textit{k}-mer distances and a Bayesian generalized
mixture model or DBSCAN to identify the clusters. In practice, the
clusters PopPUNK identifies typically correspond to clonal complexes
\textemdash an extension of MLST to a format with a central sequence
type (ST) and its closely related variants forming each complex. The
main advantage of PopPUNK is the ability to assign arbitrary reference
sequences to clusters while mostly conforming to the MLST
definitions. Additionally, the method allows for the inclusion of the
more variable accessory genome, making PopPUNK ideal for defining the
reference clusters for mSWEEP.

\subsection{Sequence alignment}
\label{section:sequence-alignment}

The sequence alignment utilized by mSWEEP differs from traditional
alignment methods that report the location a read aligns to in a
reference sequence. Instead of this location-based alignment, mSWEEP
makes use of pseudoalignments, where only a 0 or 1 is reported for
whether the read aligns somewhere within the reference sequence or
nowhere. The mSWEEP publication used the kallisto method, which
originally introduced pseudoalignments, but the mGEMS paper introduced
a more scalable version of kallisto, called Themisto, that also
provides an exact version of the pseudoalignment
algorithm. Pseudoalignments have the advantage that they are much
quicker to evaluate, enabling alignment against orders of magnitude
larger reference collections in a short amount of time. The
disadvantage is binarizing the alignments, which requires some changes
in the likelihood function of the mixture models that have previously
been proposed in the similar settings covered earlier. The next
section will cover these changes and introduce the mSWEEP and mGEMS
methods in more detail.

\section{A probabilistic model for sequences from mixed sources}
\label{section:model}

In this section I will introduce and cover the probabilistic model
used by mSWEEP. This model is an extension of the probabilistic model
for grouped reference sequences that is used by BIB. Compared to BIB,
which requires a single representative sequence from each reference,
the mSWEEP model allows for using an arbitrary number of reference
sequences to represent each lineage. In addition, to improve
scalability of the alignment step in BIB, the mSWEEP model is based on
using pseudoalignments rather than the traditional location-based
alignments. The use of many representative sequences leads to a
significantly improved accuracy when dealing with species that exhibit
variability within the reference lineages, while leveraging
pseudoalignments enables the use of much larger reference sequence
collections.

\subsection{Mixture model formulation}

Assume that we have some set of sequencing reads $R = \left\{r_{1},
\dots, r_{N}\right\}, N \in \mathbb{N}_{+}$ that are independent of
each other and identically distributed. We can write down a generative
mixture model for these reads by defining latent indicator variables
$I = \left\{I_{1}, \dots, I_{N}\right\}$ that follow some mixing
proportions $\boldsymbol{\theta} = \left(\theta_{1}, \dots,
\theta_{S}\right), S \in \mathbb{N}_{+}, \sum_{s = 1}^{S} \theta_{s} =
1$. Since we assumed independence between the reads $R$, $r_{j}
\indept r_{i}$ for all $i \neq j, 1 \leq i \leq N, 1 \leq j \leq N$, the joint
distribution for this generative model is
\begin{equation}
  \label{model:joint-distribution}
  p\left(R, I, \boldsymbol\theta\right) = \prod_{n = 1}^{N}p\left(r_{n} \middle| I_{n}\right) p\left(I_{n} \middle| \boldsymbol\theta\right)p\left(\boldsymbol\theta\right).
\end{equation}

Now, assume that for each read $r_{n}, 1 \leq n \leq N$ we observe
alignments $r_{n, s}$ against the reference sequences $s = 1, \dots,
S$. The information contained in $r_{n, s}$ may be anything about the
alignment. Now, further assuming independence between the alignments
against different reference sequences $r_{n, i} \indept r_{n, j}$ for
all $i \neq j, 1 \leq i \leq S, 1 \leq j \leq S$, the joint
distribution from Equation \ref{model:joint-distribution} factorizes
into
\begin{equation}
  \label{model:joint-distribution-factorized}
  p\left(R, I, \boldsymbol\theta\right) = p\left(\boldsymbol\theta\right)\prod_{n = 1}^{N} \prod_{s = 1}^{S} p\left(r_{n, s} \middle| I_{n} = s\right) p\left(I_{n} = s \middle| \boldsymbol\theta\right).
\end{equation}

The model in Equation \ref{model:joint-distribution-factorized}
corresponds to the mixture model that has been historically used in
many contexts, including the predecessor of mSWEEP. Note that in this
model each reference sequence represents a single taxonomic unit, and
the inferred relative abundances $\boldsymbol\theta$ are the
proportions of the same reference sequences.

\subsection{Incorporating grouped reference sequences}

Although the model in Equation
\ref{model:joint-distribution-factorized} performs admirably when the
reference sequences are sufficiently different from each other,
attempts to estimate the relative abundances of individual reference
sequences fail when the degree of relatedness is increased. Especially
when applying the model to reference data containing sequences from
strains of the same bacterial species, the abundances
$\boldsymbol\theta$ tend to become scattered among the most closely
related sequences - even if the correct sequence is contained in the
reference.

To solve this, the model used in BIB incorporated a clustering for the
reference sequences \textemdash meaning that instead of estimating the
relative abundances $\boldsymbol\theta$ for the individual reference
sequences, the abundance is estimated for the entire cluster. Although
this approach introduces an obvious loss of resolution when compared
to the sequence-based approach, introducing the use of clustering
provides advantages in accommodating for naturally occurring variation
as well as improving the scalability of the inference part by reducing
the number of reference units.

In modelling terms, Equation \ref{model:joint-distribution-factorized}
can be extended by representing alignments against the reference
sequences $r_{n, s}$ via information about alignments against a
representative sequence $r_{n, k}, k = 1, \dots, K, K \ll S$ from the
cluster $k$. With this change, the changes to the model are minimal:
the $s$:s are simply replaced by $k$:s
\begin{equation}
  \label{model:grouped-joint-distribution}
  p\left(R, I, \boldsymbol\theta\right) = p\left(\boldsymbol\theta\right)\prod_{n = 1}^{N} \prod_{k = 1}^{K} p\left(r_{n, k} \middle| I_{n} = k\right) p\left(I_{n} = k \middle| \boldsymbol\theta\right).
\end{equation}
With an appropriate definition of the likelihood term $p\left(r_{n, k}
\middle| I_{n} = k\right)$, this model (the BIB model) corresponds
to the model used by BIB and can solve the inference problem for
several species of bacteria with well-defined clusterings within the
species.

The model of Equation \ref{model:grouped-joint-distribution} has,
however, several issues that render it difficult to apply in some
scenarios. First, the model requires selecting a representative
sequence for each cluster $k$. This selection is by no means an easy
task and, secondly, using a representative sequence implies assumptions
about the clustering: namely that there must be minimal variation
within the clusters in terms of genomic contents and that each cluster
is clearly separated from the others. In BIB, the assumptions about
the clustering are solved by using a representative sequence for the
core genome of the cluster. Unfortunately this introduces a third
problem: some bacterial species do not have a stable core genome and
increasing the number of sequences for any species of bacteria tends
to shrink the core genome regardless of its characteristics due to
randomly occurring mutations.

\subsection{Modelling alignments against sequence groups}

mSWEEP solves the issues present in the BIB model by replacing
alignments against representative sequences for the clusters with a
model for alignments against all available reference sequences from
each cluster. Furthermore, since this replacement results in a large
increase in the number of sequences that are aligned against, the
traditional position-based alignment is replaced with
\textit{pseudoalignment} that only returns a $0$ or $1$ indicating
whether the read aligned somewhere within the reference sequence or
not (see Section \ref{section:sequence-alignment} for more
details). Although pseudoalignment reports less information about the
relationship between the reads and the reference sequences than
traditional alignment, the approach is observed to perform excellently
in cases where the BIB model fails and provide similar resolution in
cases where the BIB model performs well.

With the changes in the mSWEEP model, the observations $r_{n, k}$
become the numbers of observed pseudoalignments $r_{n, k}, 0 \leq
r_{n, k} \leq M_{k}$ against the $M_{k}$ sequences assigned to each
cluster $k$. If assumptions about independence between the clusters
are kept, the formulation for the model remains the same as the one
presented in Equation \ref{model:grouped-joint-distribution} with the
only changes being to the likelihood term $p\left(r_{n, k} \middle|
I_{n} = k\right)$.


\subsection{Likelihood for a clustered reference}

When dealing with pseudoalignments against clustered reference
sequences, the likelihood term $p\left(r_{n, k} \middle| I_{n} =
k\right)$ in Equation \ref{model:grouped-joint-distribution} needs to
be carefully defined to account for several factors arising from the
biological facts affecting the structure of the reference. One, the
clusters may vary greatly in size, with some of them having just one
sequence and some of them hundreds or even thousands. Two, due to
sequencing errors, reference errors (assembly errors or lack of
reference sequences from a cluster), and mutations, the read may not
necessarily pseudoalign against any sequences in a cluster even though
it belongs to the cluster. Three, the clusters are unlikely
independent of each other; and four, consequently the read can
plausibly pseudoalign against several or even all of the clusters.

These four factors lead to considering a likelihood with the following
properties: 1) within each cluster and ignoring the case where no
pseudoalignments are observed, the likelihood function must be
increasing in the number of pseudoalignments (more alignments always
means a better fit to the cluster); 2) the likelihoods from different
clusters should be on the same scale regardless of the number of
sequences in the cluster; and 3) the different clusters should have
some kind of dependency on the other clusters. This leads to us
defining the likelihood $p\left(r_{n, k} \middle| I_{n} = k\right)$ in
three parts
\begin{equation}
  \label{likelihood:without-normalization}
  p\left(r_{n, k} \middle| I_{n} = k\right) =
  \begin{cases}
    0.01\text{ if } r_{n, k} = 0, \\
    0.99\text{ if } r_{n, k} = 1 \text{ and } M_{k} = 1, \\
    0.99f\left(r_{n, k}, M_{k}\right)\text{ if } r_{n, k} \geq 1\text{ and } M_{k} > 1.
  \end{cases}
\end{equation}

In Equation \ref{likelihood:without-normalization}, the first part
provides a slight zero-inflation for the model, corresponding
(roughly) to the error rate in Illumina sequencing data with a Phred
quality score of Q20 \citep{ewing1998baseone, ewing1998basetwo}. The
second part handles the special case where the cluster $k$ contains
only one sequence ($M_{k} = 1$). For the rest of the cases, which
represent the majority of the reference sequences in a setting where
they can be plausibly assigned to clusters, the likelihood is chiefly
defined by the term $f\left(r_{n, k}, M_{k}\right)$ which is a
function of the pseudoalignment counts $r_{n, k}$ and the cluster size
$M_{k}$. In terms of our requirements for the likelihood,
$f\left(r_{n, k}, M_{k}\right)$ is the term that should fulfill them.

Had we assumed independence between the different clusters, a
reasonable choice for $f\left(r_{n, k}, M_{k}\right)$ would be the
beta-binomial distribution. This distribution is an extension of the
binomial distribution, allowing for over/under-dispersion through a
2-parameter formulation. The beta-binomial distribution has the
following probability mass function $p\left(k \middle | n, \alpha,
\beta\right)$ with parameters $\left(n, \alpha, \beta\right), n \in
\mathbb{N}, \alpha > 0, \beta > 0$ and support $k \in \left\{0, \dots,
n\right\}$
\begin{equation}
  \label{likelihood:beta-binomial-pmf}
  p\left(k \middle| n, \alpha, \beta\right) = \binom{n}{k}\frac{B\left(k + \alpha, n - k + \beta\right)}{B\left(\alpha, \beta\right)}.
\end{equation}

In Equation \ref{likelihood:beta-binomial-pmf}, $B\left(\alpha, \beta\right)$ is the beta function
\begin{equation}
  \label{likelihood:beta-function}
  B\left(\alpha, \beta\right) = \int_{0}^{1}t^{\alpha - 1}\left(1 - t\right)^{\beta - 1}dt.
\end{equation}
and $\binom{n}{k}$ is the binomial coefficient.

Since we want the clusters to have some sort of dependency on each
other, we cannot directly utilize the beta-binomial distribution in
Equation \ref{likelihood:beta-binomial-pmf} but our approach is
nevertheless inspired by it. Namely, we use a modified version of the
distribution $p^{\star}\left(k \middle| n, \alpha, \beta\right)$,
where the likelihoods are divided by their respective maximum value,
changing their range to $\left[0, 1\right]$. Note that achieving the
maximum value at $k = n$ requires restricting the parameter values of
the original beta-binomial distribution so that its probability mass
function is increasing. This assumption is fulfilled when
$\alpha\left(\alpha + \beta\right)^{-1} \in \left(0.5, 1\right)$
\citep{berg1993condorcet}.

Performing the scaling by the maximum value of $p$ in Equation
\ref{likelihood:beta-binomial-pmf} results in the following likelihood
function
\begin{equation}
  \label{likelihood:beta-binomial-scaled}
  \begin{aligned}
    p^{\star}\left(k \middle| n, \alpha, \beta\right) &= \frac{p\left(k \middle| n, \alpha, \beta\right)}{p\left(n \middle| n, \alpha, \beta\right)} \\
    &= \binom{n}{k}\frac{B\left(k + \alpha, n - k + \beta\right)}{B\left(\alpha, \beta\right)} \binom{n}{n}^{-1}\frac{B\left(\alpha, \beta\right)}{B\left(n + \alpha, \beta\right)} \\
    &= \binom{n}{k}\frac{B\left(k + \alpha, n - k + \beta\right)}{B\left(n + \alpha, \beta\right)}
  \end{aligned}
\end{equation}
Since the scaling in Equation \ref{likelihood:beta-binomial-scaled} is
by a constant ($p\left(n \middle| n, \alpha, \beta\right))$, the
resulting function $p^{\star}\left(k \middle| n, \alpha, \beta\right)$
remains an increasing function of $k$ when the original function $p$
is increasing.

%% Possible footnote
%\noindent\let\thefootnote\relax\footnote{Figure source: Adapted from \cite{praveera_fenugreek-sprouts} and \cite{niaid_escherichia-coli}. Released under the \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0 license}.}
\addtocounter{footnote}{-1}\let\thefootnote\svthefootnote
\begin{figure}[!h]
  \label{fig:msweep-vs-beta-binomial}
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{img/mSWEEP_likelihood.pdf}
    \caption{Comparison of the mSWEEP likelihood and a plain beta-binomial likelihood (without zero inflation). The figure displays the difference between the modified beta-binomial likelihood that mSWEEP uses (red dots and lines) and a plain beta-binomial likelihood (blue dots and lines) with the same parameters. Panel \textbf{a)} displays the difference for a cluster with 10 reference sequences, and panel \textbf{b)} for a cluster with 20 reference sequences.}
\end{figure}

A comparison of the likelihood presented in Equation
\ref{likelihood:beta-binomial-scaled} with the plain beta-binomial
likelihood in Equation \ref{likelihood:beta-binomial-pmf} is presented
in Figure \ref{fig:msweep-vs-beta-binomial}. Figure
\ref{fig:msweep-vs-beta-binomial} shows that the mSWEEP format gives
more weight to values of $k$ that are close to $n$ while making the
differences between the different $k$ steeper than the plain
beta-binomial. In practice, the modified format implies that the
clusters should be defined in a way that most reads will pseudoalign
to the cluster if the read originated from the cluster.

With the probability mass function of the distribution
$p^{\star}\left(k \middle| n, \alpha, \beta\right)$, the full
definition for the third part of the likelihood in Equation \ref{likelihood:without-normalization} is
\begin{equation}
  \label{likelihood:normalized}
  p\left(r_{n, k} \middle| I_{n} = k\right) = 0.99\frac{p^{\star}\left(r_{n, k} \middle| M_{k}, \alpha, \beta\right)}{Z\left(r_{n}\right)}\text{ if } r_{n, k} \geq 1\text{ and } M_{k} > 1,
\end{equation}
where $Z\left(r_{n}\right)$ is a normalizing constant that depends on
all observed pseudoalignment counts $r_{n, k}, k = 1, \dots, K$. The
scaling in Equation \ref{likelihood:beta-binomial-scaled} fulfills our
requirement that the likelihood of each cluster must be on the same
scale despite different size. The next section will derive a closed
form for the normalizing constant $Z\left(r_{n}\right)$.

\subsection{Normalizing the likelihood}

Although the function $p^{\star}\left(k \middle| n, \alpha,
\beta\right)$ in Equation \ref{likelihood:beta-binomial-scaled}
closely resembles the probability mass function of a Beta-Binomial
distribution (Equation \ref{likelihood:beta-binomial-pmf}), the
function $p^{\star}$ itself does not sum to $1$ over its support $k
\in \left\{ 1, \dots, K \right\}$. In order to turn the $p^{\star}$:s
into proper probability mass functions, we need the normalizing
constant $Z\left(r_{n}\right)$.

Although any distribution on a finite support can be normalized, in
many cases the normalizing constant does not have a closed form. In
our case it turns out that \textemdash thanks to the properties of the
beta function (see Equation \ref{likelihood:beta-function} for the
definition) \textemdash $Z\left(r_{n, k}\right)$ has a closed form. To
derive the closed form, we need the following identity for the beta
function
\begin{theorem}
  \label{lemma:beta-function-identity}
  \[
  B\left(a + 1, b\right) = \frac{a}{a + b}B\left(a, b\right).
  \]
\end{theorem}

\begin{proof}
  Follows from $B\left(a, b\right) = \frac{\Gamma\left(a\right)\Gamma\left(b\right)}{\Gamma\left(a + b\right)}$ \citep{artin_einfuhrung} and $\Gamma\left(z + 1\right) = z\Gamma\left(z\right), \text{ for all } z > 0$ \citep{davis_leonhard}, where $\Gamma\left(z\right)$ is the gamma function. Using these two identities, we can write $B\left(a + 1, b\right)$ as
  \begin{align*}
    B\left(a + 1, b\right) &= \frac{\Gamma\left(a + 1\right)\Gamma\left(b\right)}{\Gamma\left(a + b + 1\right)} \\
    &= \frac{a\Gamma\left(a\right)\Gamma\left(b\right)}{\left(a + b\right)\Gamma\left(a + b\right)} \\
    &= \frac{a}{a + b}B\left(a, b\right).
  \end{align*}
\end{proof}

Using Lemma \ref{lemma:beta-function-identity}, we can find a closed form for the normalizing constant $Z\left(k\right)$.
\begin{theorem}
  \label{theorem:likelihood-can-be-normalized}
  Let
  \[
  f\left(k, n_{k}\right) = \binom{n_{k}}{k}\frac{B\left(\alpha + k, n_{k} - k + \beta\right)}{B\left(\alpha + n_{k}, \: \beta\right)}, 0 < k < n_{k}, \: \alpha > 0, \: \beta > 0,
  \]
  \text{and}
  \[
  Z\left(r_{n}\right) = \prod_{k = 1}^{K}Z\left(n_{k}\right) = \prod_{k = 1}^{K}\prod_{j = 1}^{n_{k}}\frac{\alpha_{k} + n_{k} + k - j}{\alpha_{k} + \beta_{k} + 2n_{k} - j},
  \]
  \text{then}
  \[
  \prod_{k = 1}^{K} \frac{f\left(k, n_{k}\right)}{Z\left(n_{k}\right)} = 1.
  \]
\end{theorem}

\begin{proof}
  Consider a Beta-Binomial distribution with the parameters $\left(n, \alpha + n, \beta\right), n \in \mathbb{N}_{0}, \alpha > 0, \beta > 0$. This distribution has the probability mass function $g : 0, \dots, n \rightarrow \left[0, 1\right]$, where
  \[
  g\left(k \: \middle| \: n, \alpha + n, \beta\right) = \binom{n}{k} \frac{B\left(\alpha + n + k , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)},\: 0 \leq k \leq n.
  \]
  Using the identity $B\left(a + 1, b\right) = B\left(a, b\right)\frac{a}{a + b}, a > 0, b > 0$ (Lemma \ref{lemma:beta-function-identity}) we can derive an alternative form for $g$:
\begin{align*}
  g\left(k\right) &= \binom{n}{k} \frac{B\left(\alpha + n + k , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)} \\
  &= \binom{n}{k} \frac{B\left(\alpha + n + k - 1 , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)} \frac{\alpha + n + k - 1}{\alpha + n + k - 1 + n - k + \beta} \\
  &= \binom{n}{k} \frac{B\left(\alpha + n + k - 1 , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)} \frac{\alpha + n + k - 1}{\alpha + \beta + 2n - 1} \\
  &= \binom{n}{k} \frac{B\left(\alpha + n + k - 2 , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)} \frac{\alpha + n + k - 1}{\alpha + \beta + 2n - 1} \frac{\alpha + n + k - 2}{\alpha + \beta + 2n - 2} \\
  &= \binom{n}{k} \frac{B\left(\alpha + n + k - 2 , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)} \prod_{j = 1}^{2} \frac{\alpha + n + k - j}{\alpha + \beta + 2n - j}.
\intertext{Repeadetly applying Lemma \ref{lemma:beta-function-identity} $n$ times yields}
  g\left(k\right) &= \binom{n}{k} \frac{B\left(\alpha + n + k - n , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)} \prod_{j = 1}^{n} \frac{\alpha + n + k - j}{\alpha + \beta + 2n - j} \\
  &= \binom{n}{k} \frac{B\left(\alpha + k , n - k + \beta\right)}{B\left(\alpha + n, \beta\right)} \prod_{j = 1}^{n} \frac{\alpha + n + k - j}{\alpha + \beta + 2n - j} \\
  &= f\left(k, n\right) \prod_{j = 1}^{n} \frac{\alpha + n + k - j}{\alpha + \beta + 2n - j}.
\end{align*}
Since $g\left(k\right)$ is a probability mass function, this implies that
\[
f\left(k, n\right)\left(\prod_{j = 1}^{n}\frac{\alpha + n + k - j}{\alpha + \beta + 2n - j}\right)^{-1}
\]
is also a probability mass function. Furthermore, since the products of (independent) probability mass functions are probability mass functions, then
\[
\prod_{k = 1}^{K}f\left(k, n_{k}\right)\left(\prod_{j = 1}^{n}\frac{\alpha + n_{k} + k - j}{\alpha + \beta + 2n_{k} - j}\right)^{-1}
\]
is also a probability mass function. Thus, since we assumed that the reads $r_{n}$ are independent of each other, setting
\[
Z\left(r_{n}\right) = \prod_{k = 1}^{K} \prod_{j = 1}^{n_{k}}\frac{\alpha_{k} + n_{k} + k - j}{\alpha_{k} + \beta_{k} + 2n_{k} - j}
\]
is sufficient to normalize $f\left(r_{n, k}, n_{k}\right)$ and prove Theorem \ref{theorem:likelihood-can-be-normalized}.
\end{proof}

\subsection{Likelihood hyperparameters}

Instead of the traditional parametrization for the beta binomial
distribution through $\alpha >0, \beta > 0$, we use a
reparametrization that slightly changes the interpretation of the
parameters. The transformed parameters are
\begin{equation}
  \label{likelihood:reparametrization}
  \pi = \frac{\alpha}{\alpha + \beta}, \phi = \frac{1}{\alpha + \beta},
\end{equation}
where the first parameter $\pi$ has the range $\pi \in \left(0,
1\right)$ and represents the mean success rate in repeated draws from
the beta binomial distribution, and the second parameter $\phi > 0$
measures the variation in the success rate for each draw
\citep{griffiths1973maximum}. In our formulation for the likelihood,
each cluster $k$ has its own parameters $\pi_{k}, \phi_{k}$.

Although methods such as Bayesian optimization could be employed to
find optimal values for the parameters $\pi_{k}, \phi_{k}$ in Equation
\ref{likelihood:reparametrization}, we set their values based on a
reasonable compromise that performed well in the mSWEEP paper. The
values of $\pi, \phi$ are set to
\begin{equation}
  \begin{aligned}
    \pi_k &= 0.65, \text{ for all } k = 1, \dots, K, \\
    \phi_{k} &= 1 - \pi_{k} + 0.01M_{k}^{-1}.
  \end{aligned}
\end{equation}

\subsection{Fitting the model using variational inference}

With the likelihood defined in Equations
\ref{likelihood:without-normalization} and
\ref{likelihood:normalized}, all that remains is to come up with a
suitable method to infer the relative abundances
$\boldsymbol\theta$. Since our model is principally the same as the
one used in BIB (Equation \ref{model:grouped-joint-distribution}),
just with a different formula for the likelihood term $p\left(r_{n, k}
= k \middle| I_{n} = k\right)$, we can adapt the variational inference
used in BIB to our needs. Since this requires only changing the
likelihood term, the core of the inference algorithm remains the
same. Since variational inference is an extremely broad topic outside
of the scope of this dissertation, I will only cover the parts that
are relevant to the original contributions in the thesis. For a more
thorough coverage of the variational inference algorithm in this
context, readers are referred to the BitSeqVB paper \citep{hensman2015fast} where the
algorithm is derived for the same model with a different likelihood
function.

In brief, variational inference for the mSWEEP model consists of
finding a distribution $q\left(\boldsymbol\theta, I\right)$ that
minimizes the Kullback-Leibler divergence to the true posterior
$p\left(\boldsymbol\theta, I \middle| R\right) \approx
q\left(\boldsymbol\theta, I\right)$. For simplicity, we assume that
the approximation $q\left(\boldsymbol\theta, I\right)$ factorizes into
$q\left(\boldsymbol\theta, I\right) =
q\left(\boldsymbol\theta\right)q\left(I\right)$. Since we assumed that
each $I_{n}$ has a categorical distribution
$Cat\left(\boldsymbol\theta\right)$ and are independent of each other,
$I_{n} \indept I_{m}, n \neq m$, the second term $q\left(I\right)$
simplifies to
\begin{equation}
  \label{likelihood:vi-factorization}
  q\left(I\right) = \prod_{n = 1}^N\prod_{k = 1}^K \gamma_{n, k}^{I_{n, k}}.
\end{equation}

In practice, the best approximation $q\left(\boldsymbol\theta,
I\right)$ is found when optimal values for the parameters $\gamma_{n,
  k}$ in Equation \ref{likelihood:vi-factorization} are found (TODO
cite). We use the Riemannian conjugate gradient (RCG) method to find
the optimal values for $\gamma_{n, k}$ (TODO cite). A generic and
parallel \& distributable implementation for arbitrary likelihood with
this mixture model structure is available from
\url{https://github.com/tmaklin/rcgpar}.

\subsection{Alternative fit using MCMC sampling}
Alternatively, the model could be fitted using Markov chain Monte
Carlo (MCMC) sampling methods (TODO citations or explain). Instead of
finding an approximating distribution $q\left(\boldsymbol\theta,
I\right)$, MCMC sampling attempts to produce a set of samples from the
true posterior $p\left(\boldsymbol\theta, I\middle|
R\right)$. Averaging over the values produced via MCMC sampling
$\hat{\boldsymbol\theta}$ asymptotically produces the true parameters
$\boldsymbol\theta$ as the number of samples $n_{MCMC}$
increases. Even though producing the true values is tempting instead
of using the values from the variational approximation, MCMC has
several problems when applied in practice (TODO citations). First, the
true values are only found asymptotically, meaning that it is
difficult to measure when the sampling has been performed for long
enough for the chain to have converged to the true
posterior. Secondly, due to the first point, the number of samples
that need to be drawn may be excessively high, resulting in extremely
long run times when compared to the faster variational inference (TODO
cite).

MCMC does however have the advantage that, provided sufficient
runtime, the samples will be from the true posterior. When comparing
MCMC to VI in practice, the former has been reported to better capture
correlations between the sampled parameters (TODO cite Antti's bitseq
paper). Because of this, we compared the VI from mSWEEP to a version
of mSWEEP compatible with the MCMC sampling from BitSeq (TODO cite
BitSeq) but found no significant differences between the two (TODO figure).

\textbf{TODO: - Add figure of Gibbs sampler vs variational inference experiments.}

\section{From profiling to binning}
\label{section:binning}

This sections covers using the model from mSWEEP to provide an
algorithm for binning sequencing reads. Binning differs from the
abundance estimation, or taxonomic profiling, in that the goal is to
produce some assignment of the reads to reference units. In typical
cases, bins correspond to some individual species or genuses, but in
this thesis binning will be performed on the level of the clusters
that mSWEEP uses for reporting the relative abundances. Compared to
merely doing taxonomic profiling, binning provides a much higher level
of detail about the contents of a sample, since many read-based
downstream analyses may be performed once the bins have been
constructed. Contrary to the section presenting the probabilistic
model, the binning algorithm is quite straightforward.

The work in this section is based on the results from the second paper
included in the dissertation \citep{maklin_bacterial_2021}.

\subsection{The mGEMS binning algorithm}

A crucial feature for the binning algorithm to handle lineage-level
differences between bacteria is the possibility for a single read to
belong to multiple bins at the same time. This differs from most of
the work on binning, where the reads are typically only assigned to a
single bin and the variation between the bins is assumed large enough
that this approach is correct. For strain-level-binning to work, this
assumption is obviously not correct, since the differences between the
genomes can be minimal. Thus, the binning algorithm must be derived
with this feature in mind.

The mGEMS binning algorithm is derived by leveraging the read-level
assignment probabilities $\gamma_{n, k} \in \left(0, 1\right), n = 1,
\dots, N, k = 1, \dots, K, \sum_{k = 1}^{K}\gamma_{n, k} = 1$ produced
by the variational approximation (Equation
\ref{likelihood:vi-factorization}). We assume that the sequencing
reads do not contain several members of the same cluster and that,
should the true reference sequence that generated the reads be missing
from the reference, the set of reference sequences adequately captures
the variation present in the cluster. To fulfill our requirement that
the read can belong to several bins at the same time, we define a bin
$G_{k}$ for each cluster $k$ as a subset of sequencing reads $r_{n}$
such that
\begin{equation}
  \label{binning:bins-definition}
  G_{k} = \left\{r_{n} : \gamma_{n, k} \geq q_{k}\right\}
\end{equation}
holds for a threshold $q_{k} \in \left[0, 1\right]$. The threshold may
be different for each cluster $k$. Because of the way the bins $G_{k}$
are defined in Equation \ref{binning:bins-definition}, this definition
obviously allows for the reads to belong to several bins (for a
trivial example, consider the case where $q_{k} = 0$ for all $k$).

\subsection{Assignment rule for multi-cluster membership}

Next, we need to find a sensible value for the thresholds $q_{k}$ that
ideally maximizes the probability $A_{n, k}$ of assigning the read
$r_{n}$ to the bin $G_{k}$ if the cluster $k$ (could have) generated
the read $r_{n}$. Ideally, we would define the probability $A$ through
another probability $B_{n, k}$: the cluster $k$ contains a sequence
that contains the true (error-free) nucleotide sequence of the read
$r_{n}$. However, the probability $B$ is difficult to estimate
directly since 1) the reads cannot be error-corrected with full
accuracy, and 2) the reference collection is non-complete. However, by
assuming that the sample is composed mostly of closely related
organisms, we can derive the following relationships between $A_{n,
  k}$ and $B_{n, k}$
\begin{equation}
  \label{binning:a-is-approximately-b}
  P\left[A_{n, k} = 1\right] \approx \theta_{k}P\left[J_{n, k} = 1\right],
\end{equation}
where $\theta_k$ is the relative abundance of the cluster $k$ in the
sample that generated the reads $r_{n}$. Equation
\ref{binning:a-is-approximately-b} implies that when $P\left[A_{n, k}
  = 1\right] \geq \theta_{k}$, then $P\left[B_{n, k} = 1\right]$ must
be ``large'' for the approximation to hold. A more detailed derivation
for this statement about the magnitude of $B_{n, k}$ and Equation
\ref{binning:a-is-approximately-b} is provided in the methods section
of the mGEMS article \cite{maklin_bacterial_2021}, supplied in the
appendix for this thesis, and thus omitted from here.

The implied statement about the magnitude of $B_{n, k}$ when
$P\left[A_{n, k} = 1\right] \geq \theta_{k}$ means that there is a
high chance that $B_{n, k} = 1$ when the former holds. Through this,
we can derive an assignment rule by using the estimates $\gamma_{n,
  k}$ (Equation \ref{likelihood:vi-factorization}) for the probability
$P\left[A_{n, k} = 1\right]$
\begin{equation}
  \label{binning:theoretical-assignment-rule}
  \text{if } \gamma_{n, k} \geq \theta_{k}, \text{ assign the read } r_{n} \text{ to } G_{k}.
\end{equation}
Equation \ref{binning:theoretical-assignment-rule} provides us with an
inequality whose validity can be checked to assess the probability of
the event $B_{n, k} = 1$ which we could not estimate directly.

\subsection{Practical considerations}

While the assignment rule in Equation
\ref{binning:theoretical-assignment-rule} provides a theoretically
sound tool to assign reads $r_{n}$ to the bins $G_{k}$, applying it in
practice requires a slight adjustment due to computational
accuracy. Namely, when estimating the relative abundances $\theta_{k}$
of $N$ reads, any estimate that falls below $\frac{1}{N}$ means that
zero reads originated from the cluster $k$. Because of this, values
$\theta_{k} < \frac{1}{N}$ are meaningless and all represent the same
case of 0 reads from the cluster. Due to the constraint that
$\theta_{k}$ must sum up to $1$ over $k$, these essentially-zero
values do, however, contribute a small amount of noise to the
estimates. Since there are $K$ clusters, the fraction of noise $d$ is
(in the worst-case scenario) at most
\begin{equation}
  \label{binning:assignment-rule-noise}
  d = K\frac{1}{N}.
\end{equation}

Equation \ref{binning:assignment-rule-noise} means that when
evaluating the validity of the inequality presented in Equation
\ref{binning:theoretical-assignment-rule}, we should adjust the
threshold $\theta_k$ with $1 - d$, meaning that we consider only the
fraction of relative abundance that is assigned to nonzero
estimates. This produces the final assignment rule that is used in
mGEMS
\begin{equation}
  \label{binning:assignment-rule}
  \text{if } \gamma_{n, k} \geq (1 - d)\theta_{k}, \text{ assign the read } r_{n} \text{ to } G_{k}.
\end{equation}

Since the variational approximation used to fit the mSWEEP model
already provides both the estimates $\gamma_{n, k}$ and the relative
abundances $\theta_{k}$, the assignment rule in Equation
\ref{binning:assignment-rule} is in practice extremely inexpensive to
evaluate after the model has been fitted.

Since the relative abundances $\theta_k$ are derived from the values
$\gamma_{n, k}$ by averaging over $n = 1, \dots, N$, the assignment
rule in Equation \ref{binning:assignment-rule} can be seen as a way to
cluster the rows (or columns) of a generic probability matrix while
allowing assigning each row (or column) to several clusters at the
same time. We do not, however, explore the applicability of this rule
to other scenarios where a probability matrix should be clustered in
this manner. Nevertheless, the rule is general enough to acknowledge
its potential applications elsewhere.

The next chapter will cover the application of mSWEEP and mGEMS to
both different kinds of sequencing data and explore how the methods
can be applied to enable new directions in analysis of sequencing
data. The chapter also includes a brief coverage of the benchmarks and
experiments presented in the first and second papers (TODO cite).

\chapter{High-resolution metagenomics}

High-resolution metagenomics (in the context of this thesis) refers to
applying some method capable of recovering variation at the
within-species level to some sort of metagenomics data as defined in
Section \ref{three-approaches-to-metagenomics}. In our case, the
methods of interest are mSWEEP and mGEMS. This chapter will deal with
the benchmarking and experimental results from the first two papers,
which focus on analysing plate sweep metagenomics data. At the end of
the chapter some technicalities arising from the model formulating will
also be covered \textemdash namely questions about reliability of the
results when applied to realistic use-cases and sensitivity to
parameter changes.

\section{Plate sweep and WGS metagenomics}

The primary methods for obtaining metagenomics sequencing data
considered in this thesis are plate sweep metagenomics and
whole-genome shotgun (WGS) metagenomics (see Section
\ref{three-approaches-to-metagenomics} for more details). Of these
two, plate sweep metagenomics has the advantage of being able to focus
sequencing efforts to species that are known to grow on specific
plates, while WGS metagenomics provides data of \textit{all} organisms
on some sample (including for example host DNA, fungi, commensal
species). When the goal is to investigate strain-level variation, both
approaches have their uses in either producing more data from the
species of interest or providing a less biased view of the sample
contents at the expense of sequencing depth. The first two papers
included in the thesis were written with only plate sweep metagenomics
in mind but the third paper demonstrated that mSWEEP and mGEMS are
applicable in the WGS metagenomics context. Thus, this chapter will
not discriminate between the two.

\subsection{Benefits of metagenomics over culturing}

When comparing metagenomics approaches to culturing, the obvious
difference is that the former will provide a better view of the
microbiome in the sample. Although epidemiological analyses in the
past have mostly focused on using isolate data, some studies showing
the benefits of using a metagenomics approach have emerged. In
particular, a recent study utilizing mSWEEP and mGEMS showed that
using isolate sequencing data alone can result in an approach that
dismisses the presence of non-dominant variants of the same species
\citep{tonkin-hill_pneumococcal_2022}. Similar approaches using other
tools have also hinted at the capability of several strains to become
pathogenic should the opportunity present itself (TODO
citations). These studies have demonstrated that it may be necessary
to incorporate some level of metagenomics in epidemiological studies
in order to better identify the causative agents (TODO citations) of
disease.

Microbiome studies in general have become increasingly popular with
the advent of 16S sequencing. Although the results of some studies
remain somewhat controversial, the widespread application of this
method has revealed extensive variation in the microbiome contents
sampled form comparable sources. It is currently an open question
whether this variation extends to the strain-level but nevertheless
there exist studies both old and new that suggest this to be the
case.

mSWEEP and mGEMS specifically enable answering questions about
strain-level variation by providing a method that can be targeted to
identify the variation in some species. Since many species of clinical
importance have been studied for several decades with significant
sequencing efforts aimed at them to obtain high-quality genome
assemblies, the reference-based approach employed by mSWEEP and mGEMS
is ideal for disentangling variation in the same setting. When
combined with tools with other kinds of approaches to sequencing, the
methods together have the potential for unprecedented level of detail
in epidemiological analyses and tracking

While the previous chapter covered the theoretical foundations of
mSWEEP and mGEMS, the rest of this chapter will focus on evaluating
some practical considerations regarding the methods. Namely, we
briefly overview the performance of the methods when compared to other
established approaches, and cover some questions related to the
reliability of the approach as well as the sensitivity to the
parameter choices. The last part of the chapter will cover some
results from the included papers related to what kind of information
metagenomics-derived results provide.


\subsection{The mSWEEP/mGEMS pipeline}

\textbf{TODO: - Flowchart describing the analysis pipeline.}

Combining the results from Sections \ref{section:model} and
\ref{section:binning} produces the complete workflow for analysing
sequencing data from mixed sources. This workflow is originally
presented in the second paper \citep{maklin_bacterial_2021}, where it
is referred to as the mGEMS pipeline.

\subsubsection{Constructing the reference database}

The pipeline begins with constructing a set of reference sequences
that represent the variation in the target species of interest. In an
ideal scenario, the reference should consist of high-quality
assemblies from each lineage that is expected to be found in the
sequencing reads. Since this is a rather unrealistic assumption,
typical cases make use of published datasets and possible combine
isolate sequencing data from their own studies. Either previously
published assemblies or even curated genomes from databases such as
RefSeq may be used but the reference may also include newly assembled
sequences or otherwise be tailored to the problem at hand. In both
cases a cautious approach is recommended, as the quality of the
reference sequence set is the most important factor in obtaining
trustworthy results from the pipeline.

After the appropriate reference sequences have been collected, they
should be clustered in some meaningful way to obtain the grouping. For
some species, multilocus sequence typing is sufficient but for others
with more variable genome contents, algorithms that attempt to
identify clonal complex analogues may be useful. One such algorithm is
PopPUNK, which is demonstrated to perform relatively well in the third
paper included in this thesis. PopPUNK clusters the reference
sequences based on accessory and core genome distances, and provides
results that are very similar to clonal complexes (groups of STs) for
species where the complexes are defined. Using a computational like
PopPUNK instead of a curated database approach like sequence types and
clonal complexes has the advantage of being able to assign sequences
that have not yet been included in the curated databases, or work with
species for which such databases do not exist.

After clustering the reference sequences, the next step is to build an
index for pseudoalignment, and pseudoalign the reads from the samples
against the index. In the mGEMS pipeline, we use Themisto to perform
both the index construction and the pseudoalignment. The
pseudoalignment step produces the alignment vectors for each read
against every reference sequence, which are used as the input to
mSWEEP.

\subsubsection{Estimating relative abundances and binning the reads}

The next step in the pipeline is to use mSWEEP to estimate the
relative abundances of the reference groups based on the
pseudoalignments against the sequences. This is performed directly on
the output from Themisto, with no intermediate steps required. After
the relative abundances have been estimated, the results are fed to
mGEMS which produces the read bins and optionally also extracts the
reads corresponding to each bin from the original set.

\subsubsection{Assembling the read bins}

In the second article of this thesis, the mGEMS pipeline also includes
an optional step to assemble the sequences. The suggested assembler is
shovill, which is an assembly pipeline built around the spades
assembler but incorporating some pre- and post-processing
steps. Naturally other assemblers may also be used, or the assembly
step skipped entirely and the analysis instead focusing the reads. In
the second article we mostly focused on analysing the assemblies, as
including an assembly step introduces a post-processing step that can
be useful in filtering out reads that have been mistakenly assigned to
the bin.

Since mGEMS allows for including a sequencing read in several bins at
once, the produced read bins may have a very high coverage for shared
parts of the genome when the sample contains several closely related
organisms. Because of this, we investigated the effect of replacing
the isolate-data optimised shovill with metagenomic assemblers, which
presumably implement better handling of variable coverage in the
produced genomes. Although we observed some differences in the
resulting assemblies, there was no statistically significant evidence
in favour of using either approach. Regardless of the assembler
choice, using the mGEMS pipeline produced assemblies in downstream
analyses performed similarly to a benchmark consisting of assemblies
derived from isolate sequencing data.

\subsubsection{Quality control}

The mGEMS pipeline as described above is the method that has been
applied in the third paper with the additional inclusion of a
post-processing step aimed at identifying whether the reference
sequences suitably cover the reads or not. This post-processing step,
demix\_check (\url{https://github.com/harry-thorpe/demix_check/}),
performs several quality control checks on the results from mSWEEP and
mGEMS, aiming to determine whether the created read bins correspond to
a reference cluster or not. Although this step was not included in the
two methods papers presenting mSWEEP and mGEMS, its inclusion
addresses an important question regarding the applicability of the
results from mSWEEP/mGEMS. Therefore, including demix\_check
\textemdash or other similar approach \textemdash as part of the mGEMS
pipeline (between the mGEMS and the assembly steps) is recommended for
a rigorous approach. This and other questions related to quality
control and reliability of the results are explored further down in
this chapter.

\textbf{TODO:- figure from showing the threshold construction process.}

\subsection{Other approaches for metagenomic analyses}
\label{other-metagenomics-approaches}
While this thesis deals with the development and usage of mSWEEP and
mGEMS, one has to acknowledge that the use of metagenomic sequencing
data is by no means an understudied field. In fact, dozens of methods
exist that aim to perform similar tasks ranging from genome assembly
from metagenomic sequencing data (metagenome assemblers) to taxonomic
binning (metagenomic binners) and profiling, and strain tracking
(StrainGE and Strainphlan). Compared to mSWEEP and mGEMS, these
methods typically assume that the samples only contain a single strain
from each species, which makes the task solvable. While it may at
first seem suspicious that a thorough comparison between these methods
and mSWEEP/mGEMS has not been included in the published papers, the
assumptions about the strain complexity lie at the heart of the
problem. Since these tools have not been developed with the kind of
data that mSWEEP/mGEMS analyse in mind, a performance comparison
between them is not meaningful.

\section{Benchmarking mSWEEP and mGEMS}
This section will briefly cover the results related to benchmarking
the performance of mSWEEP and mGEMS in the first and the second
papers. A majority of the benchmarks were performed on synthetic
mixtures of sequencing reads from isolate cultures but the mGEMS paper
also provides a (significantly smaller in scale) benchmark that used
\textit{in vitro} mixtures of DNA from isolate cultures. Figures from
the papers have not been reproduced here, and the results are
presented in a summarizing format merely to convince the reader of the
kinds of applications that mSWEEP and mGEMS perform well in.

\subsection{mSWEEP}
The first paper in the thesis presented a comparison of mSWEEP with
the Bayesian Identification of Bacteria (BIB) and the
pseudoalignment-based metakallisto methods. Although a myriad of
taxonomic profilers have been developed (these will be covered in more
detail in Section \ref{other-metagenomics-approaches}), the vast
majority of them do not attempt within-species profiling or have been
developed for cases with only one strain present from each
species. Because of these limitations, the mSWEEP paper only makes the
comparisons with BIB and metakallisto, which have been developed for
settings with several strains present.

While the approach in BIB is similar to mSWEEP in that the reference
sequences are grouped together and estimation is performed on the
level of these groups, metakallisto attempts the much more ambitious
task of estimating the relative abundances for the individual
sequences. Because of this, a direct comparison between the three is
not possible. We addressed this issue by modifying the output from
metakallisto by including a step that sums over the abundance
estimates within the same group. Even though this step was not
included in the original metakallisto paper, its addition helps to
compare the estimates from mSWEEP, BIB, and metakallisto.

The main result of the first paper is that mSWEEP vastly outperforms
both BIB and metakallisto for most species. For species with a
strictly defined clonal structure \textemdash such as
\textit{S. aureus} \textemdash, BIB and metakallisto are able to derive
comparable performance, but for other species that exhibit more varied
clonal structures incorporating the probabilistic model from mSWEEP
proves to be a necessary step in obtaining accurate
information. Although these performance benchmarks were only performed
on data containing a single strain in each sample, we demonstrate in
the paper through stochastic dominance that the methods which do not
succeed with single-strain estimates are unlikely to provide accurate
results from multi-strain samples.

Another benchmark presented in the first paper concerns evaluating
performance in the presence of several strains from the same
species. In this benchmark, sequence data from three strains were
mixed together in single sample at known proportions, and mSWEEP was
applied to estimate the proportions when the real reference sequences
were absent from the reference data but close representatives from the
same lineage were available. In this benchmark, mSWEEP demonstrates
very accurate performance measured on both true positives and true
negatives. Although \textit{K. pneumoniae} in particular proved
challenging, possibly due to the wide variety exhibited in the
reference set, the results were nevertheless within acceptable error

In addition to the synthetic experiments presented in the first paper,
the second paper presents an \textit{in vitro} benchmark concerning
abundance estimation. In this benchmark DNA from three different
strains of either \textit{E. coli} or \textit{E. faecalis} was mixed
together using Qbit, producing samples where the initial composition
of the sample is known at the DNA level. This benchmark shows similar
results as the synthetic benchmarks for \textit{E. faecalis}, where
the three strains originated from three different sequence types, but
also shows promise in differentiating between strains of the same
clade within a single sequence type of \textit{E. coli}. This task is,
however, much, much more difficult since the split of the ST131
subclade C2 was based on incorporating information from the accessory
genome using PopPUNK. The accessory genome of \textit{E. coli} is not
as stable as the core genome, on which the ST131 and the ST131 A-C
designations are based, resulting in some difficulties in separating
the C2-4 and C2-6 sublineages. Nevertheless mSWEEP manages to identify
the presence of both clades quite well, and the downstream analysis
using mGEMS (covered in Section \ref{mgems-performance-benchmark}
still provides accurate results.

\subsection{mGEMS}
\label{mgems-performance-benchmark}

Our taxonomic binner mGEMS was benchmarked in the second paper using
both synthetic data (mixtures of reads from isolate cultures of
different strains) as well as the previously mentioned \textit{in
  vitro} benchmark. The chief metrics used were related to those that
are the main objects of interest in genomic epidemiological analyses:
SNPs and phylogenies estimated from the SNP data. We looked at the
performance of mGEMS on the level of within-ST variation using data
from \textit{E. coli} (TODO cite paper), at the between-ST and
within-species level using \textit{E. faecalis}, and an extreme case
with only dozens of SNPs separating the different reference clusters
using data from \textit{S. aureus}.

\subsubsection{Synthetic data benchmarks}

In the \textit{E. coli} benchmark we investigated how well
mGEMS-derived assemblies performed for maximum likelihood phylogeny
estimation (using RAxML-NG, Gamma+GTR4M model) when compared to using
isolate sequencing data. We used snippy to derive the core genome
alignment against the same reference genome for both mGEMS-derived
assemblies and the isolate assemblies. The results showed that mGEMS
tends to slightly overestimate the number of SNPs in these assemblies
(Figure 2 panel a in the paper (TODO cite)) but the phylogenetic
relationships (Figure 3) are recovered quite well. This benchmark was
performed at the level of variation within a sequence type
(\textit{E. coli} ST131).

The \textit{E. faecalis} benchmark was performed similarly to the
\textit{E. coli} one but with the intention of investigating
performance with between-ST variation. Additionally,
\textit{E. faecalis} is known to have a relatively high rate of
recombination within the species across ST lines (TODO citation),
which adds to the difficulty of the problem. Nevertheless, the mGEMS
derived-assemblies do recover the overall structure of the phylogeny
and place to sequences from the same ST to the same clade. While the
global structure is somewhat different from the isolate assembly
phylogeny, these kind of differences may easily be explained by
uncertainty arsing from recombination affecting the placement of the
STs globally. This phenomenon is also apparent in the bootstrap
support values for both the isolate and mGEMS-derived phylogenies.

The final synthetic benchmark in the second paper investigated
phylogeny recovery in \textit{S. aureus} ST22 sublineages that are
separated by a few dozen SNPs (TODO citation). Although the
performance in this benchmark was not quite as good as in the two
others, the mGEMS-derived results nevertheless do manage to replicate
some parts of the results relating to the transmission analysis
performed in the source study for this data. Namely, the samples that
were observed to be the likely source of the pathogenic strain in the
sequenced patients were placed at the root of the phylogeny when using
both mGEMS and isolate data. Regardless, there is some lack of detail
further down in the tree which is likely a result of the small degree
of separation between the sublineages.

\subsubsection{\textit{In vitro} benchmark}

The second paper also included an \textit{in vitro} benchmark data,
where known amounts of DNA from three different strains were mixed
together using Qbit, and both the mixed sample and the corresponding
isolate cultures were sequenced. This data was used to re-test the
performance of both mSWEEP (covered in the previous subsection) and
mGEMS. The mGEMS part of the test examined the recovery of SNPs from
either the isolate sequencing data or the mixed sequencing data. In
both the \textit{E. coli} and \textit{E. faecalis} test samples the
SNPs recovered from the mGEMS data reflect the ``true'' values from
the isolate data quite closely, although there is some difficulty in
separating the ST131-C2 sublineages 4 and 6. However, due to the
increasingly difficult task of defining sublineages within
sublineages, this difficulty is likely not relevant for practical
analyses.

Together, these benchmarks hopefully show that the mGEMS method can be
reliably used to disentangle metagenomic sequencing data and create
lineage-specific read bins. These bins in turn can be used in standard
epidemiological analyses in place of isolate sequencing data,
producing similar results. While mGEMS does not completely replace the
use of isolate sequencing data in epidemiology \textemdash the
availability of high-quality reference sequences from isolate
sequencing remains a critical part of the pipeline \textemdash the
method nevertheless shows promise in reducing the number of isolate
cultures that need to be created. Additionally, applying mGEMS to pure
metagenomics sequencing data can yield results that previously
available tools have not been able to produce as will be shown in more
in Chapter \ref{section:metagenomic-epidemiology} that covers the
experimental results from the third paper.

\section{Assessing reliability of the results}

A principal question when applying mSWEEP/mGEMS in practice is whether
the results from either are trustworthy or not. Since the former
method uses a Bayesian approach to estimating the relative abundances
of the reference lineages, none of the abundances will ever be exactly
zero. This naturally results in the question: ``When is a lineage
truly present in the sample?'' when applying mSWEEP in
practice. Furthermore, since the abundance estimates from mSWEEP are
the basis for the mGEMS pipeline, it is of importance to know when the
results for lineages with a high (in some sense) relative abundance
are truly correct and not just a result of, for example, missing
reference lineages. These two questions will be covered in the rest of
this section.

\subsection{Detection thresholds for lineages}
To answer the question regarding the minimum abundance required to
call an abundance estimate reliable or not, the mSWEEP paper provides
an approach termed \textit{detection thresholds}. These thresholds aim
to provide a minimum abundance that the estimates must exceed in order
to be reliable, and give accompanying p-value-analogues to assess the
degree of trust in the threshold itself. Although this approach is
computationally somewhat cumbersome to apply in practice, especially
when the size of the reference sequence set increases, the approach
should nevertheless provide means for more careful consideration of
the estimates without resorting to simpler measures such as filtering
by a constant abundance.

The detection threshold approach is based on bootstrapping sequencing
reads from existing samples with known genome assemblies in the
reference collection. The assembly is removed from the reference, and
the bootstrapped reads (which contain data from only the single,
removed assembly) are put through mSWEEP to obtain a bootstrapped
abundance estimate. This approach is then repeated for some number of
reference sequences, each removed in turn, to provide several
bootstrapped abundance estimates for each reference lineage. The
bootstrapped estimates are used to evaluate the magnitude of the
relative abundance estimates that are given for the \textit{absent}
lineages, and combined across all values generated when varying the
removed reference sequences. When repeated for all reference lineages,
this approach gives a set of values that represent the false estimates
when the true contents of the sample are different. The false estimates
for each lineage are ordered, and one of the higher values is picked
as a cutoff point that defines the detection threshold for this
lineage with an accompanying p-value-analogue that depends on the
number of estimates that are allowed to exceed the cutoff. This
somewhat complicated approach is described in more detail in the first
paper.

While the detection thresholds approach has some merit in its
theoretical foundations in using bootstrapping to estimate the
magnitude of the false estimates, the approach has some problems when
applied in practice. Namely, the need to remove a reference sequence
to bootstrap the abundance estimates necessitates reconstruction of the
pseudoalignment index for each sample that is generated from the
removed sequences. Since dynamic indexing (meaning that sequences can
be added/removed without rebuilding the whole index) remains an open
problem at the time of writing, the approach is quickly rendered
computationally infeasible when applied to reference sets that are
larger than those used in the mSWEEP paper. Unfortunately for the
detection threshold construction, for many applications it is tempting
to include as much reference data as possible to cover naturally
occurring strain-level variation. This has resulted in the approach
being not applied in practice and remaining a curiosity until dynamic
indexes hopefully become available.

\subsection{Pseudocoverage as a threshold}

One alternative to the detection thresholds can be found by figuring
out a way to define some sort of threshold on the abundance estimates
without utilizing bootstrapping-based approaches. In practice, this
translates to using the number of aligned reads and the abundance
estimates together to estimate the \textit{pseudocoverage} of the
reference lineage. Pseudocoverage $c^{\star}_{k}$ for reference
lineage $k$ is defined as the product of the abundance estimate
$\theta_{k}$ for lineage $k$ and the total number of bases $b$ in the
reads that pseudoaligned against any reference sequence divided by the
average genome length $l_{k}$ in lineage $k$
\begin{equation}
  \label{pseudocoverage}
  c_{k}^{\star} = \frac{\theta_{k}b}{l_{k}}.
\end{equation}
Although the definition in Equation \ref{pseudocoverage} is similar to
the basic definition of average coverage (number of bases in aligned
reads divided by genome length), these two definitions are not
necessarily the same.

To elucidate the difference between coverage and pseudocoverage,
consider a set of sequencing reads originating from two bacterial
strains, both with $100$ base pair long genomes, that differ by a
single base pair and are present in equal proportions in the sample
providing the sequencing reads. Assuming that we have 100 one base
pair long reads from each position in both genomes, the average
coverage of both strains would be $199$ since $9900\cdot2$ reads
will align to both genomes and only $100\cdot2$ reads will align to
just one. However, when considering the relative abundance estimates
for these two strains, their true values are $0.50$ and $0.50$ since
we know that both strains are present in equal proportions. This gives
both strains a pseudocoverage of $99.5$ \textemdash a conservative
value compared to the traditional coverage definition.

Pseudocoverage can be used to define a threshold on the relative
abundance estimates by finding the value of $\theta_{k}$ that provides
for example a pseudocoverage of \textit{at least} $1$x. Since the
pseudocoverage is a conservative estimate of the true coverage (the
exact, typically more complex than in the thought experiment above,
relationship depending on the relatedness of the lineages $k$), this
minimum value can be taken as a sort of a threshold on the relative
abundances. If combined with means to investigate whether the lineages
that remain after filtering by this approach are a good fit to the
reference lineages, pseudocoverage provides an adjustable rule that is
more easy to evaluate than the detection thresholds described above.

\subsection{Compatibility of the clustering and the reads}
The remaining question regarding the reliability of the estimates from
mSWEEP concerns the fit of the reference lineages with the estimated
contents of the bins produced by mGEMS. This issue was not covered in
neither the mSWEEP nor the mGEMS paper but since the publication of
the first, an external method has been developed to address the
question. This method, called demix\_check (TODO citation), uses mash
(TODO citation) distances between the bins from mGEMS and the
reference sequences assigned to each lineage to evaluate the fit
between the read bin and the lineage. In practice, demix\_check has
proved invaluable in addressing the results from mGEMS and is an
integral part of the processing pipeline that was used in the third
paper of this thesis. Since this method was not developed by the
author of this thesis, it won't be covered in more detail although its
inclusion in the mGEMS pipeline is strongly recommended.

%% \subsection{Brief consideration of model assumptions}
%% %% * Not needed *
%%
%% For a reader more versed in probabilistic modelling, a final question
%% regarding reliability would relate to the assumptions made by the
%% mSWEEP model and when and whether those assumptions hold or not.
%%
%% - Zero inflation vs no inflation; degree of inflation
%% - Sensitivity to hyperparameter values

\chapter{Metagenomic epidemiology}
\label{section:metagenomic-epidemiology}

(TODO citations) Genomic epidemiology refers to the use of sequencing
data to identify pathogen transmissions chains and analyse the spread
and diversity of the pathogen population. These analyses are typically
performed using cultivated colonies, where the bacterial strains of
interest are isolated and sequenced from their own plates in order to
produce sufficiently deep sequencing depth for SNP calling, assembly,
and other genomic analyses. In the same spirit, metagenomic
epidemiology refers to performing the genomic epidemiology tasks but
forgoing the culture step and using for example WGS metagenomics to
identify and analyse the pathogens. While in the previous literature
metagenomic epidemiology has typically been performed using genus- or
species-level resolution tools like 16S sequencing, in this thesis the
term will also encompass the use of mSWEEP/mGEMS to perform the
analyses at the within-species level.

\section{From genomic epidemiology to metagenomic}

In the previous chapter we saw that mSWEEP and mGEMS enable
high-resolution analysis of metagenomic sequencing data. Especially
when it comes to epidemiologically relevant analyses such as SNP
calling and assembly, the mGEMS-derived read bins perform nearly
identical to isolate sequencing data. This means that most standard
epidemiological analyses can be performed with metagenomic sequencing
data by applying the mGEMS pipeline \textemdash provided that a
sufficiently accurate reference database exists for the species of
interest. Using metagenomic sequencing data in place of isolate
sequencing data comes with the previously covered cost-efficiency and
vast expansion of throughput. Metagenomic epidemiology also enables
performing various novel analyses by allowing strain-level analysis of
either the unbiased data produced by WGS metagenomics, or detailed
exploration of the diversity within some restricted set of taxons that
can be enriched via the plate sweep metagenomics approach.

This shorter chapter will briefly cover some of the experimental metagenomic
epidemiology results from the three papers included in this thesis as
well as the advantages and disadvantages of incorporating metagenomic
sequencing data. The first paper included an experiment with
real-world plate sweep metagenomics data, and the third paper provides
an example of applying the mGEMS pipeline to WGS metagenomics
data. While the second paper did not include a real-world application,
the \textit{in vitro} mixture samples analysed in that paper do
highlight some challenges for the methods that are relevant to this
chapter. The chapter ends with some speculation about the future
applicability of the methods and the types of analyses that are
possible with the introduction of mSWEEP and mGEMS.

\subsection{Metagenomics-derived results}

Epidemiological analyses that have been performed from metagenomic
sequencing data have the major advantage of covering the full spectrum
of bacteria in a sample without the bias introduced by using
cultivation steps. Because of this, results derived from metagenomic
data should (in theory, with sufficiently high sequencing depth) be
capable of providing at least the same results as non-metagenomics
approaches while vastly extending the possibilities with regards to
interactions between non-pathogenic and pathogenic species, and to
identifying and tracking non-dominant strains of pathogenic
species. When it comes to applying these methods in practice and
interpreting the results, there are some obstacles standing in the way
of rendering isolate sequencing studies redundant.

One of the most obvious questions in using metagenomics-derived
results in practice is what to do with the diversity that can be found
in most samples. For practical use, most of the species (especially
the commensal ones) are firstly not of any interest from a clinical
point of view. Secondly, when dealing with microbiomes that are less
extensively studied, many of the strains will not correspond to any
previously sequenced lineages or even species, rendering the
reference-based approaches like mSWEEP and mGEMS useless. Even for
reference-free approaches, it can be difficult to place the results in
a meaningful context if the samples contain significant amounts of
previously unencountered diversity. These factors mean that when
talking about metagenomic epidemiology in practice, the interpretation
and analyses might typically focus on species that have already been
studied using isolate sequencing, and exhaustive exploration of the
diversity remains somewhat unfeasible but the approach has
nevertheless potential.

\subsection{Challenges}
The major challenges in using metagenomic sequencing data relate to
the diversity found in the samples. It is not untypical to encounter
the (pathogenically) interesting species at very low abundances,
resulting in a need to sequence the sample at much higher depth than
what would be used in isolate studies. This low abundance can also
cause problems in terms of identifying the presence of the species in
the first place as the number of reads that uniquely map to the
species may be even lower than its relative abundance. Therefore, many
sequencing reads will be generated from commensal or even contaminant
species which usually have no use in the downstream analyses. In
addition, WGS metagenomics sometimes results in an overabundance of
host DNA in the sample, which dominates the contents of the reads from
a sequencing run.

A second problem with metagenomics analyses relates to the lack of
reference data from many domains of life that might be found in the
uncultivated sample. Even if we disregard the presence of for example
fungi, yeasts, and bacteriophages that are commonly found alongside
the interesting bacteria, restricting ourselves to the bacterial
domain still leaves a massive number of bacteria that have not been
studied before. Although the estimates for the amount of ``microbial
dark matter'' vary depending on the source (TODO citations), the
discovery of even completely new genera is not unheard of. This
presents significant challenges for reference-based methods if the
goal is to analyse the full diversity of the sample. While focusing on
the more well-known bacteria does help in resolving the issue, new
species of these bacteria are nevertheless occasionally
discovered. When going further down to the within-species level, it is
almost a certainty to find new lineages of the species when the
sequencing effort is sufficiently large.

The third issue is related to the lack of maturity in methods for
analysing metagenomic data, perhaps connected to the difficulties in
solving the previous issues. Although this thesis attempts to peddle
mSWEEP and mGEMS as tools for partially solving the problem, they are
by and far not capable of performing \textit{all} kinds of analyses
that one might be interested in and come with the acknowledged issues
related to the reference-based approach. In practice, this implies
that for many samples a human intervention in analysis pipelines may
be required to identify cases that are plagued by these issues, and to
remove them from further consideration \textemdash  a daunting and
difficult task.

\subsection{Advantages}

Although using WGS or plate sweep metagenomics in practice has its
disadvantages, some major advantages in favour of the approaches do
(obviously) exist, the foremost of these the ability to analyse the
complete diversity with a single sequencing run. When performing a
metagenomic sequencing run, we get vastly more data about the contents
and composition of the sample than what would be obtained even with
several different culture media and subsequent isolate sequencing
runs. This information in turn enables making inferences about the
coexistence and competition dynamics between different taxonomic units
or even cotransmission when considering epidemiological applications.

Another advantage of metagenomics-based analyses is their capability
to increase the number of samples that can be processed since the
plating steps may be entirely skipped. If we are not interested in
obtaining high sequencing depths, then the samples may simply be
processed through the WGS metagenomics pipeline and disentangled
computationally. For a higher depth, the plate sweep approach may be
used to generate more reads from some interesting species. Regardless,
both approaches significantly reduce the amount of laboratory work
that is needed and (when they work) allow processing of samples even
in less-well resourced facilities.

\section{Metagenomic epidemiology in practice}

While the previous section covered theoretical factors related to
using metagenomics-derived results in practice, this section will
focus on briefly summarizing the practical application of mSWEEP and
mGEMS to both plate sweep and WGS metagenomics data in the included
papers. The first subsection will cover results from the plate sweep
approach that was initially employed and required by both mSWEEP and
mGEMS. The second subsection shows results from applying the two
methods to WGS metagenomics data, showing that the methods are not
restricted to the more specialized plate sweep approach. The results
are naturally presented in more detail in their respective papers but
a summary is provided here to elaborate on the potential applications
of mSWEEP and mGEMS.

\subsection{Plate sweep metagenomics}

In the mSWEEP paper, the method was applied to a set of \textit{in
  vitro} plate sweep samples from children sequenced at Vietnamese
hospitals. These samples were paired for information about before and
after exposure to antibiotic treatment in diarrhea cases, and the
whole plate was sequenced in accordance to the plate sweep protocol
suggested in the paper. The samples were analysed with mSWEEP, and in
the paper we compared the differences in \textit{E. coli} sequence
type contents and their relative abundances.

The results from these samples found a significant difference in the
lineage composition between the two sets of samples, with the
commensal ST10 and ST14 complexes being much more common in the
pre-treatment set, and the hospital-associated ST131 and ST38 (TODO
citations) being more common post-treatment. No significant difference
was detected in the composition of the samples (magnitudes of the
relative abundance estimates), meaning that there was no significant difference in
the number of samples that contained coexisting lineages pre- or post-treatment.

This analysis demonstrates simple means to analyse the
lineage-contents of some sets of samples using only the relative
abundance estimates. While it would be optimal to include the mGEMS
pipeline steps (unavailable at the time the article was written), the
results obtained using only abundance estimates are in line with the
hypothesis/knowledge that commensal lineages may be replaced
antibiotic-resistance harbouring lineages because of treatment. If a
more thorough sampling had been performed, the abundance estimates
alone would have been enough to identify when or if the lineage
composition shifts back to the previous, presumably stable,
composition found in the pre-treatment cohort. Focusing the analysis
on the \textit{E. coli} diversity only using the plate sweep approach
provided high enough detail in the sequencing reads that this sort of
within-lineage exploration could be performed. With the development of
mGEMS, the analysis would become even more powerful with the
possibility to separate the different strains (in cases that exhibited
coexistence) and provide means to run for example antibiotic
resistance gene finders or phylogenetic analyses confidently.

\subsection{WGS metagenomics}

The third paper shows a set of results from applying mSWEEP and mGEMS
to WGS metagenomics sequencing data. This data was obtained from
another study that investigated the differences in colonization of the
newborn human gut using WGS metagenomic data from the 21 days of
life. In the original study the analyses were performed only on the
species-level. We used this dataset to demonstrate that mSWEEP and
mGEMS can be used to provide additional insights into the strain-level
dynamics present in the samples by focusing on the species that are
known to be pathogenic and have extensive available reference
collections.

The results from this paper elucidate the colonisation dynamics in a
virginal microbiome, showing a strong competition for the first strain
of a species to colonize the gut. We found that in very few cases the
samples contained several strains of the same species. Instead, the
babies would be colonized by a single strain that typically persisted
in the gut for the first 21 days or in some cases switched to another
strain. Coexistence (within species) was rarely observed across the
species analysed although we found some synergistic relationships
between \textit{Klebsiella} species that the original study had
missed. Additionally, we used the mGEMS-derived assemblies to perform
analyses of the antimicrobial gene contents of the different species,
finding no significant differences between the vaginally born and
caesarean section delivered babies.


All of these analyses required the use of mSWEEP and mGEMS, as to our
knowledge no other methods exist for analysing this kind of data at
this kind of resolution. This study, together with the plate sweep
studies, demonstrates the usefulness of having a method capable of
targeted, high-resolution analysis of some parts of the
microbiome. Additionally, since these results were derived from WGS
metagenomics data, we demonstrated the removal of a barrier in
requiring plate sweep metagenomics data that has prevented more
widespread application of mSWEEP and mGEMS. This shows that our two
methods can used alongside established tools for metagenome analyses
when more information is desired about some particular subset of
organisms within the samples.

\section{Conclusions}

In this last chapter of the thesis we have demonstrated some analyses
enabled by the application of mSWEEP and mGEMS to either plate sweep
or WGS metagenomics sequencing data. Although the first two included
papers had been written with only plate sweep sequencing data in mind,
the third paper provides hopefully convincing evidence of the quality
of results that can be derived from WGS metagenomics. We hope that the
methods will find their place in the set of tools available for
analysis of bacterial sequencing data, and look forward to exploring
the exiting possibilities in the field of both genomic and metagenomic
epidemiology that they enable.

\printbibliography[heading=bibintoc]

\chapter*{Included papers\markboth{Included papers}{}}
\addcontentsline{toc}{chapter}{Included papers}

\subsection*{Paper I \textemdash High-resolution sweep metagenomics using fast probabilistic inference}
By \underline{Tommi MÃ¤klin}, Teemu Kallonen, Sophia David, Christine J
Boinett, Ben Pascoe, Guillaume MÃ©ric, David M Aanensen, Edward J Feil,
Stephen Baker, Julian Parkhill, Samuel K Sheppard, Jukka Corander, and
Antti Honkela. Published in \textit{Wellcome Open Research} (2021), 5:14
(https://doi.org/10.12688/wellcomeopenres.15639.2).

\subsection*{Paper II \textemdash Bacterial genomic epidemiology with mixed samples}
By \underline{Tommi MÃ¤klin}, Teemu Kallonen, Jarno Alanko, Ãrjan
Samuelsen, Kristin Hegstad, Veli MÃ¤kinen, Jukka Corander, Eva Heinz,
and Antti Honkela. Published in \textit{Microbial Genomics} (2021)
7.11 (https://doi.org/10.1099/mgen.0.000691).

\subsection*{Paper III \textemdash Strong pathogen competition in neonatal gut colonization}
By \underline{Tommi MÃ¤klin}, Harry Thorpe, Anna PÃ¶ntinen, Rebecca
Gladstone, Alan McNally, Ãrjan Samuelsen, PÃ¥l Johnsen, Trevor Lawley,
Antti Honkela, and Jukka Corander. Awaiting peer-review; available
from \textit{bioRxiv} (2022) (https://doi.org/10.1099/mgen.0.000691).

%\include{papers/maklin2022_thesis-papers}

\end{document}
